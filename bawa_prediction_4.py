# -*- coding: utf-8 -*-
"""BAWA_Prediction_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u1Er5S5TUVHhexmSsxKkaN1VCAJa6puq
"""



import pickle

model_file_path = '/content/trained_xgboost_model.pkl'  # Update this to the correct path

with open(model_file_path, 'rb') as file:
    xgboost_model = pickle.load(file)

print(df.columns)

# Updated list of features to drop
features_to_drop = ['timestamp', 'home_team_name', 'away_team_name']

# Drop the features
df = df.drop(features_to_drop, axis=1)

# Define your features (X) and target variable (y)
X = df.drop('FTR_encoded', axis=1)  # Assuming 'FTR_encoded' is your target column
y = df['FTR']



from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable
y_encoded = label_encoder.fit_transform(y)

# Now y_encoded contains the encoded labels as integers

import pandas as pd

# Load the data
file_path = '/content/cleaned_matches_data_with_features.csv'  # Path to your file
df = pd.read_csv(file_path)

# Display the first few rows to confirm it's loaded correctly
print(df.head())

cutoff_date = '2023-11-30'
train_data = df[df['date_GMT'] < cutoff_date]
test_data = df[df['date_GMT'] >= cutoff_date]

"""Results Visualization
Visualize the results of the model.
"""

# Drop columns that won't be used as features
columns_to_drop = ['timestamp', 'date_GMT', 'status', 'referee', 'stadium_name', 'league']
X_train = train_data.drop(columns=columns_to_drop + ['FTR'], axis=1)
y_train = train_data['FTR']
X_test = test_data.drop(columns=columns_to_drop + ['FTR'], axis=1)
y_test = test_data['FTR']

import pandas as pd

# Load your dataset
df = pd.read_csv('/content/cleaned_matches_data_with_features.csv')

# Convert 'date_GMT' to a datetime object
df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Define your cutoff date
cutoff_date = pd.Timestamp('2023-11-30')

# Split the data
train_data = df[df['date_GMT'] < cutoff_date]
test_data = df[df['date_GMT'] >= cutoff_date]

# Check if data splitting is correct
print("Training data shape:", train_data.shape)
print("Testing data shape:", test_data.shape)

# Proceed with feature selection and model training

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Selecting features for the model
feature_columns = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
target_column = 'FTR'  # Replace with your target column

# Splitting the features and target
X_train = train_data[feature_columns]
y_train = train_data[target_column]
X_test = test_data[feature_columns]
y_test = test_data[target_column]

# Defining preprocessing for numeric and categorical features
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Creating a pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', LogisticRegression())])

# Training the model
pipeline.fit(X_train, y_train)

# Predicting on the test set
y_pred = pipeline.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")





# Assuming 'cleaned_matches_data_with_features.csv' is loaded into 'df'
# Replace 'features_list' with the list of feature column names you want to include
features_list =  ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
X = df[features_list]
y = df['FTR']  # Or your target column

# Now run the cross-validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(model, X, y, cv=5)

print("Cross-validation scores:", cv_scores)
print("Mean CV score:", cv_scores.mean())



print("Training Features:", X_train.columns.tolist())
print("Testing Features:", X_test.columns.tolist())



# Step 1: Compare feature lists
print("Features in X_train:", X_train.columns.tolist())
print("\nFeatures in X_test:", X_test.columns.tolist())

# Step 2: Identify and add missing features in X_test
missing_features = [feature for feature in X_train.columns if feature not in X_test.columns]
for feature in missing_features:
    X_test[feature] = 0  # or any other appropriate default value

# Step 3: Align feature order in X_test to match X_train
X_test = X_test[X_train.columns]

# Step 4: Check and align data types if necessary
for column in X_train.columns:
    if X_train[column].dtype != X_test[column].dtype:
        X_test[column] = X_test[column].astype(X_train[column].dtype)

# Now try predicting again
try:
    y_pred = model.predict(X_test)
    print("Prediction successful.")
except Exception as e:
    print("Error during prediction:", e)

# If the prediction is successful, you can proceed with evaluation
if 'y_pred' in locals():
    print(classification_report(y_test, y_pred))

# Train the model with the current set of features in X_train
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# Check the distribution of classes in the test set
print(y_test.value_counts())



from google.colab import drive
drive.mount('/content/drive')

import os

folder_path = '/content/drive/My Drive/94_betting_model'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# Now save the model
joblib.dump(model, folder_path + '/94_betting_model.pkl')

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)
# Similarly for other DataFrames or relevant objects

print(cleaned_matches_data_df.head())

folder_path = '/content/drive/My Drive/94_betting_model'  # Replace with your folder path

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)

# Predict on the test set
y_pred = model.predict(X_test)

# Add predictions to the test DataFrame
test_data['Predicted_FTR'] = y_pred

# Filter the DataFrame for the specific dates
games_on_nov_30 = test_data[test_data['date_GMT'].dt.date == pd.to_datetime('2023-11-30').date()]
games_on_dec_01 = test_data[test_data['date_GMT'].dt.date == pd.to_datetime('2023-12-01').date()]

# Display the predictions for these dates
print("Predictions for Games on November 30, 2023:")
print(games_on_nov_30[['home_team_name', 'away_team_name', 'Predicted_FTR']])

print("\nPredictions for Games on December 01, 2023:")
print(games_on_dec_01[['home_team_name', 'away_team_name', 'Predicted_FTR']])



"""# ***PREDICTIONS!!!!!!***"""

cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches_data_df.csv')

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)

folder_path = '/content/drive/My Drive/94_betting_model'



# Check for unique values or formats in 'date_GMT'
print(df['date_GMT'].unique())

# Option 1: Drop rows where 'date_GMT' is not in a standard date format
# df = df[df['date_GMT'] != 'No Date present']

# Option 2: Replace non-standard dates with a default date or NaN
# df['date_GMT'] = df['date_GMT'].replace('No Date present', pd.NaT)  # or some default date

# After cleaning, convert 'date_GMT' to datetime
X['date_GMT'] = pd.to_datetime(X['date_GMT'], errors='coerce')  # 'coerce' will set invalid parsing as NaT

# Convert 'date_GMT' to datetime and then extract numerical features
X['date_GMT'] = pd.to_datetime(X['date_GMT'], errors='coerce')
X['year'] = X['date_GMT'].dt.year
X['month'] = X['date_GMT'].dt.month
X['day'] = X['date_GMT'].dt.day
X = X.drop('date_GMT', axis=1)  # Drop the original 'date_GMT' column

# Ensure 'FTR' is not in your features DataFrame
X = X.drop('FTR', axis=1, errors='ignore')



# Convert 'date_GMT' to datetime, setting errors to 'coerce' will turn invalid parsing into NaT
df['date_GMT'] = pd.to_datetime(df['date_GMT'], errors='coerce')

# Drop rows where 'date_GMT' is NaT (not a time)
df = df.dropna(subset=['date_GMT'])

from sklearn.metrics import classification_report

# Now, use classification_report to evaluate your model
print(classification_report(y_test_encoded, y_pred))

print(y_train.value_counts())
print(y_test.value_counts())



# Extract year, month, and day as separate features
X_train['year'] = X_train['date_GMT'].dt.year
X_train['month'] = X_train['date_GMT'].dt.month
X_train['day'] = X_train['date_GMT'].dt.day

X_test['year'] = X_test['date_GMT'].dt.year
X_test['month'] = X_test['date_GMT'].dt.month
X_test['day'] = X_test['date_GMT'].dt.day

# Drop the original 'date_GMT' column
X_train = X_train.drop('date_GMT', axis=1)
X_test = X_test.drop('date_GMT', axis=1)

model.fit(X_train, y_train)



# Convert 'date_GMT' to datetime and then extract year, month, and day
X_train['year'] = X_train['date_GMT'].dt.year
X_train['month'] = X_train['date_GMT'].dt.month
X_train['day'] = X_train['date_GMT'].dt.day
X_train = X_train.drop('date_GMT', axis=1)  # Drop the original 'date_GMT' column

# Repeat for X_test if you have a test set
X_test['year'] = X_test['date_GMT'].dt.year
X_test['month'] = X_test['date_GMT'].dt.month
X_test['day'] = X_test['date_GMT'].dt.day
X_test = X_test.drop('date_GMT', axis=1)

model.fit(X_train, y_train)

# Assuming X_train and y_train are already defined and preprocessed
model.fit(X_train, y_train)

# Assuming X_test and y_test are your testing data
y_pred = model.predict(X_test)

# Evaluate the model's performance
print(classification_report(y_test, y_pred))

import pandas as pd

# Assuming 'model' is your trained XGBoost model
feature_importances = model.feature_importances_

# Assuming 'X_train' is your training data with feature names
feature_names = X_train.columns

# Create a DataFrame to display feature names and their importances
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort the DataFrame by importance in descending order
importances_df = importances_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(importances_df)

df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Splitting data: Training data is before November 28th
train_data = df[df['date_GMT'] < '2023-11-28']

# Testing data is on or after November 28th
test_data = df[df['date_GMT'] >= '2023-11-28']

# Assuming 'FTR' is the target variable
X_train = train_data.drop(['FTR', 'date_GMT'], axis=1)  # Dropping 'date_GMT' if not used as a feature
y_train = train_data['FTR']

X_test = test_data.drop(['FTR', 'date_GMT'], axis=1)
y_test = test_data['FTR']

model.fit(X_train, y_train)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("First few rows of X_train:\n", X_train.head())

import pandas as pd

# Load your data into a DataFrame
df = pd.read_csv('/content/cleaned_matches_data_with_features.csv')

# Data cleaning and preprocessing steps
# ...

# Convert 'date_GMT' to datetime for filtering
df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Split the data
train_data = df[df['date_GMT'] < '2023-11-28']
test_data = df[df['date_GMT'] >= '2023-11-28']

# Prepare features and labels
X_train = train_data.drop(['FTR', 'date_GMT'], axis=1)  # Assuming 'FTR' is your target
y_train = train_data['FTR']
X_test = test_data.drop(['FTR', 'date_GMT'], axis=1)
y_test = test_data['FTR']

categorical_cols = ['status', 'home_team_name', 'away_team_name', 'referee',
                    'home_team_goal_timings', 'away_team_goal_timings',
                    'stadium_name', 'league', 'head_to_head']

df = pd.get_dummies(df, columns=categorical_cols)



print(df.columns)

selected_features = [
    'home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win',
    'home_team_possession', 'home_team_shots', 'over_25_percentage_pre_match',
    'team_a_xg', 'average_corners_per_match_pre_match', 'away_team_goal_count',
    'away_team_shots_off_target', 'away_team_goal_count_half_time', 'home_ppg',
    'away_team_shots_on_target', 'home_team_fouls', 'draws_head_to_head',
    'average_home_possession', 'home_team_form', 'Home Team Pre-Match xG',
    'odds_ft_over45', 'Pre-Match PPG (Home)', 'Pre-Match PPG (Away)',
    'Game Week', 'home_team_shots_off_target', 'average_cards_per_match_pre_match'
]



print(df.columns)

selected_features = [
    'home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win',
    'home_team_possession', 'home_team_shots', 'over_25_percentage_pre_match',
    'team_a_xg', 'average_corners_per_match_pre_match', 'away_team_goal_count',
    'away_team_shots_off_target', 'away_team_goal_count_half_time', 'home_ppg',
    'away_team_shots_on_target', 'home_team_fouls',
    'Home Team Pre-Match xG', 'odds_ft_over45', 'Pre-Match PPG (Home)',
    'Pre-Match PPG (Away)', 'Game Week', 'home_team_shots_off_target',
    'average_cards_per_match_pre_match'
]

X = df[selected_features]
y = df['FTR']  # Replace with your actual target column name

# Continue with model training and evaluation
# ...

import pandas as pd

# Assuming 'df' is your DataFrame
# List all columns that are of object type, which are typically considered categorical
categorical_features = df.select_dtypes(include=['object']).columns

# Print the list of categorical features
print("Categorical Features:", list(categorical_features))

import pandas as pd

# Convert 'birthday_GMT' to datetime and extract year
df['birthday_GMT'] = pd.to_datetime(df['birthday_GMT'])
df['birth_year'] = df['birthday_GMT'].dt.year

# One-hot encode low cardinality categorical features
df = pd.get_dummies(df, columns=['league', 'season', 'position', 'age_group'])

# Label encode high cardinality features
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in ['full_name', 'Current Club', 'nationality', 'player_id']:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

# Drop the original 'birthday_GMT' column
df = df.drop('birthday_GMT', axis=1)

# Now df is ready for model training



# Assuming 'df' is your DataFrame
print(df.head())

from sklearn.metrics import classification_report

# Now you can use classification_report without any error
print(classification_report(y_test_encoded, y_pred))

df

# Assuming your dataframe is named 'df' and the date_GMT column is named 'date_GMT'
# Replace 'start_date' and 'end_date' with your specific date range

start_date = '2023-11-28'
end_date = '2023-12-10'

# Filter the dataframe based on the date range
df = df[(df['date_GMT'] >= start_date) & (df['date_GMT'] <= end_date)]

# Display the filtered dataframe
df

df.head(30)

# Save the dataframe to a CSV file
df.to_csv('filtered_data.csv', index=False)

# Save the dataframe to a CSV file
df.to_csv('filtered_data.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('filtered_data.csv')



# Commented out IPython magic to ensure Python compatibility.
# List only the variables that are DataFrames
# %whos

import pickle

# Correct path to your saved XGBoost model
xgboost_model_path = '/content/drive/My Drive/trained_xgboost_model.pkl'

# Load the XGBoost model
try:
    with open(xgboost_model_path, 'rb') as file:
        xgboost_model = pickle.load(file)
    print("XGBoost model loaded successfully.")
except FileNotFoundError:
    print("Model file not found. Please check the file path.")
except Exception as e:
    print(f"An error occurred while loading the XGBoost model: {e}")



# Filter the DataFrame for matches on November 28th and 29th
nov_28_29_matches = df[df['date_GMT'].str.contains('Nov 28 2023|Nov 29 2023', na=False)]

# Display the first few rows to verify
print(nov_28_29_matches.head())

# Prepare the feature set for prediction
X_nov_28_29 = nov_28_29_matches[features]  # Assuming 'features' is a list of your model's feature names

# Verify the prepared data
print(X_nov_28_29.head())

with open(model_file_path, 'wb') as file:
    pickle.dump(model, file)



print(df.columns)

PREDICTIONS CONTINUED

"""PREDICTIONS CONTINUED"""



# Assuming 'df' is your DataFrame
df['FTR'] = (df['home_team_goal_count'] - df['away_team_goal_count']).apply(
    lambda x: 'H' if x > 0 else ('D' if x == 0 else 'A')
)

# Feature columns (excluding target-related columns)
feature_columns = [col for col in df.columns if col not in ['home_team_goal_count', 'away_team_goal_count', 'FTR']]

# Target column
target_column = 'FTR'

# Split the DataFrame into features (X) and target (y)
X_train = df[feature_columns]
y_train = df[target_column]

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'FTR' column to numerical classes
df['FTR_encoded'] = label_encoder.fit_transform(df['FTR'])

# Define y_train using the encoded FTR column
y_train = df['FTR_encoded']

print("Unique values in y_train:", set(y_train))

import pickle

# Assuming your trained model is stored in a variable named 'model'
model_file_path = '/content/drive/My Drive/trained_xgboost_model.pkl'  # Adjust the path as needed

# Save the model to a file
with open(model_file_path, 'wb') as file:
    pickle.dump(model, file)

# Selecting the required features for prediction
features_for_prediction = ['home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win']
X_new = df_new[features_for_prediction]

# Ensure X_new is preprocessed similarly to your training data
# (Add any preprocessing steps here if required)

import pickle
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to the saved model file
model_file_path = '/content/drive/My Drive/trained_xgboost_model.pkl'

# Load the model
with open(model_file_path, 'rb') as file:
    loaded_model = pickle.load(file)

!pip install xgboost
!pip install scikit-learn --upgrade

import pickle

# Paths to your model files
xgboost_model_path = '/content/drive/My Drive/trained_xgboost_model.pkl'
betting_model_path = '/content/drive/My Drive/betting_model_v1.pkl'

# Load the XGBoost model
try:
    with open(xgboost_model_path, 'rb') as file:
        xgboost_model = pickle.load(file)
    print("XGBoost model loaded successfully.")
except Exception as e:
    print(f"An error occurred while loading the XGBoost model: {e}")

# Load the betting model
try:
    with open(betting_model_path, 'rb') as file:
        betting_model = pickle.load(file)
    print("Betting model loaded successfully.")
except Exception as e:
    print(f"An error occurred while loading the betting model: {e}")



# Example: Filtering for matches on a specific date (adjust the condition as needed)
upcoming_matches = df[df['date_GMT'].str.contains('Upcoming_Date', na=False)]

# Define the file path for the new CSV file
new_data_file_path = '/content/drive/My Drive/upcoming_matches.csv'  # Adjust the path as needed

# Save the filtered data to a CSV file
upcoming_matches.to_csv(new_data_file_path, index=False)

# Load the new data for prediction
df_new = pd.read_csv(new_data_file_path)

# Prepare the features for prediction (ensure the same preprocessing as your training data)
# ...

# Load your trained model and make predictions
# ...



import shap

# Create a SHAP explainer and calculate SHAP values
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_train)

# Summary plot
shap.summary_plot(shap_values, X_train)

from sklearn.model_selection import GridSearchCV

# Define a set of parameters to test
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    # ... add other parameters here
}

# Initialize the Grid Search
grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='accuracy')

# Perform Grid Search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best parameters:", grid_search.best_params_)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# Define a distribution of parameters to test
param_dist = {
    'n_estimators': randint(100, 500),
    'learning_rate': uniform(0.01, 0.2),
    # ... add other parameters here
}

# Initialize the Random Search
random_search = RandomizedSearchCV(XGBClassifier(), param_dist, n_iter=25, cv=3, scoring='accuracy')

# Perform Random Search
random_search.fit(X_train, y_train)

# Best parameters
print("Best parameters:", random_search.best_params_)

# Assuming 'match_outcome' is your target variable
correlation_matrix = df.corr()
print(correlation_matrix['FTR'].sort_values(ascending=False))

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Selecting features and target
features = ['home_team_recent_form', 'away_team_recent_form',
            'average_home_possession', 'average_away_possession',
            # ... include other relevant features
           ]
X = df[features]
y = df['FTR']

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model = XGBClassifier()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

df.drop('home_wins_head_to_head', axis=1, inplace=True)

# Convert the 'favorite_based_on_odds' to a binary outcome similar to 'FTR'
# Assuming 'Home' win is 0 and 'Away' win or 'Draw' is 1
cleaned_matches_data_df['favorite_binary'] = (cleaned_matches_data_df['favorite_based_on_odds'] != 'Home').astype(int)

# Calculate odds accuracy
cleaned_matches_data_df['odds_accuracy'] = (cleaned_matches_data_df['favorite_binary'] == cleaned_matches_data_df['FTR']).astype(int)

def calculate_recent_form(df, team, num_games=3):
    recent_games = df[((df['home_team_name'] == team) | (df['away_team_name'] == team))].tail(num_games)
    form = 0
    for _, game in recent_games.iterrows():
        if game['home_team_name'] == team:
            if game['FTR'] == 'H':  # Assuming 'H' represents a home win
                form += 1
            elif game['FTR'] == 'A':  # Assuming 'A' represents an away win
                form -= 1
        else:
            if game['FTR'] == 'A':  # Away team wins
                form += 1
            elif game['FTR'] == 'H':  # Home team wins
                form -= 1
    return form

# Applying the function to calculate recent form
df['home_team_recent_form'] = df.apply(lambda row: calculate_recent_form(df, row['home_team_name']), axis=1)
df['away_team_recent_form'] = df.apply(lambda row: calculate_recent_form(df, row['away_team_name']), axis=1)

# Selecting categorical columns
categorical_cols = ['home_team_name', 'away_team_name', 'referee', 'stadium_name', 'league']

# Applying one-hot encoding
X_train = pd.get_dummies(X_train, columns=categorical_cols)
X_test = pd.get_dummies(X_test, columns=categorical_cols)

# Aligning columns of train and test set
X_train, X_test = X_train.align(X_test, join='inner', axis=1)  # This ensures both sets have the same dummy columns

model = XGBClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))



# Selecting categorical columns
categorical_cols = ['home_team_name', 'away_team_name', 'referee', 'stadium_name', 'league']

# Applying one-hot encoding to the entire dataset
X_encoded = pd.get_dummies(X, columns=categorical_cols)

# Now perform cross-validation
from sklearn.model_selection import cross_val_score
model = XGBClassifier()
scores = cross_val_score(model, X_encoded, y, cv=5)
print("Cross-validated scores:", scores)

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)  # Make sure X_train and y_train are already defined



import pandas as pd

# Get feature importances
feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=X_train.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)

# Display the feature importances
print(feature_importances)

# Assuming 'favorite_based_on_odds' is the column that shows the expected winner based on odds
cleaned_matches_data_df['odds_accuracy'] = (cleaned_matches_data_df['favorite_based_on_odds'] == cleaned_matches_data_df['FTR']).astype(int)

incorrect_odds_df = cleaned_matches_data_df[cleaned_matches_data_df['odds_accuracy'] == 0]
# Perform analysis on 'incorrect_odds_df' to find patterns

