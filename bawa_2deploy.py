# -*- coding: utf-8 -*-
"""BAWA_2deploy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pg6D_5xOiGt531EOpQMq-ZOt8wr0Cez9
"""

import os
import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

def load_and_preprocess_data(path_to_files, file_names):
    data_frames = {}
    for file_name in file_names:
        # Construct DataFrame name and path
        df_name = file_name.split('-')[2]  # Extracting a meaningful name, adjust as needed
        full_path = f"{path_to_files}{file_name}"
        data_frames[df_name] = pd.read_csv(full_path)

        # Example preprocessing steps applied to each DataFrame
        data_frames[df_name].fillna(0, inplace=True)  # Fill NA values, adjust as needed

    return data_frames

file_names = [
    "europe-uefa-champions-league-league-2023-to-2024-stats.csv",
    "europe-uefa-champions-league-matches-2023-to-2024-stats.csv",
    "europe-uefa-champions-league-players-2023-to-2024-stats.csv",
    "europe-uefa-champions-league-teams-2023-to-2024-stats.csv",
]
path_to_files = '/content/drive/My Drive/'  # Adjust your path accordingly
data_frames = load_and_preprocess_data(path_to_files, file_names)

import pandas as pd

def load_and_preprocess_all_leagues(path, league_names):
    all_leagues_data = []

    for league_name in league_names:
        # Check if the league is the Premier League or the Champions League for different file name patterns
        if 'premier-league' in league_name:
            file_suffix = ' (2)'
        elif 'champions-league' in league_name:
            file_suffix = ' (3)'
        else:
            file_suffix = ' (1)'

        # Load each dataset for the league
        team_stats_filename = f"{league_name}-teams-2023-to-2024-stats{file_suffix}.csv"
        player_stats_filename = f"{league_name}-players-2023-to-2024-stats{file_suffix}.csv"
        match_stats_filename = f"{league_name}-matches-2023-to-2024-stats{file_suffix}.csv"
        league_stats_filename = f"{league_name}-league-2023-to-2024-stats{file_suffix}.csv"

        # Add the full path to the filenames
        team_stats = pd.read_csv(path + team_stats_filename)
        player_stats = pd.read_csv(path + player_stats_filename)
        match_stats = pd.read_csv(path + match_stats_filename)
        league_stats = pd.read_csv(path + league_stats_filename)

        # Preprocessing steps for each league
        # Example merge (customize as per your requirement)
        merged_df = pd.merge(match_stats, team_stats, left_on='home_team_name', right_on='team_name', how='left')

        # Add more preprocessing steps as needed
        # ...

        # Append the processed DataFrame to the list
        all_leagues_data.append(merged_df)

    # Concatenate all league data into one DataFrame
    combined_df = pd.concat(all_leagues_data, ignore_index=True)

    return combined_df

# List of league names with their specific file suffixes
league_names = [
    'europe-uefa-champions-league',
    'europe-uefa-europa-league',
    'europe-uefa-europa-conference-league',
    'england-premier-league',
    'spain-la-liga',
    'germany-bundesliga',
    'france-ligue-1',
    'portugal-liga-nos',
    'scotland-premiership',
    'italy-serie-a',
    'england-championship'
    # ... add other leagues if needed
]

# Replace with your actual file path
path_to_files = '/content/drive/My Drive/94_betting_model/'

# Load and preprocess data for all leagues
combined_leagues_df = load_and_preprocess_all_leagues(path_to_files, league_names)

import os

# Path to the directory
directory_path = '/content/drive/My Drive/94_betting_model/BAWA_3_2024/'

# List all files in the directory
file_list = os.listdir(directory_path)
print(file_list)

import pandas as pd

def load_and_preprocess_all_leagues(path, league_names):
    all_leagues_data = []

    for league_name in league_names:
        # Check for specific league file naming conventions
        file_suffix = ''
        if 'champions-league' in league_name:
            file_suffix = ' (3)'
        elif 'premier-league' in league_name:
            file_suffix = ' (2)'
        else:
            file_suffix = ' (1)'

        # Load each dataset for the league
        team_stats = pd.read_csv(f'{path}/{league_name}-teams-2023-to-2024-stats{file_suffix}.csv')
        player_stats = pd.read_csv(f'{path}/{league_name}-players-2023-to-2024-stats{file_suffix}.csv')
        match_stats = pd.read_csv(f'{path}/{league_name}-matches-2023-to-2024-stats{file_suffix}.csv')
        league_stats = pd.read_csv(f'{path}/{league_name}-league-2023-to-2024-stats{file_suffix}.csv')

        # Preprocessing steps for each league
        # Example merge (customize as per your requirement)
        merged_df = pd.merge(match_stats, team_stats, on='team_key', how='left')
        # Add more preprocessing steps as needed...

        # Append the processed DataFrame to the list
        all_leagues_data.append(merged_df)

    # Concatenate all league data into one DataFrame
    combined_df = pd.concat(all_leagues_data, ignore_index=True)

    return combined_df

# List of league names based on the screenshot provided
league_names = [
    'europe-uefa-champions-league',
    'europe-uefa-europa-league',
    'europe-uefa-europa-conference-league',
    'england-premier-league',
    'spain-la-liga',
    'germany-bundesliga',
    'france-ligue-1',
    'portugal-liga-nos',
    'scotland-premiership',
    'italy-serie-a',
    'england-championship'
    # Add any other leagues if needed
]

# New file path
path_to_files = '/content/drive/My Drive/94_betting_model/'

# Load and preprocess data for all leagues
combined_leagues_df = load_and_preprocess_all_leagues(path_to_files, league_names)

print(team_stats.columns)
print(player_stats.columns)
print(match_stats.columns)
print(league_stats.columns)

# Exporting the combined_leagues_df DataFrame to a CSV file
combined_leagues_df.to_csv('/content/drive/My Drive/94_betting_model/combined_leagues_data.csv', index=False)

# Extracting specified training features
selected_features = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
training_data = combined_leagues_df[selected_features]

# Optionally, you can also export the extracted training data
training_data.to_csv('/content/drive/My Drive/94_betting_model/training_data.csv', index=False)

import pandas as pd

def load_and_preprocess_all_leagues(path):
    all_leagues_data = []

    # Assuming the corrected list of league names and their specific file naming conventions
    league_names = [
        'europe-uefa-champions-league',
        'europe-uefa-europa-league',
        'europe-uefa-europa-conference-league',
        'england-premier-league',
        'spain-la-liga',
        'germany-bundesliga',
        'france-ligue-1',
        'portugal-liga-nos',
        'scotland-premiership',
        'italy-serie-a',
        'england-championship'
    ]

    for league_name in league_names:
        file_suffix = '(1)' if league_name != 'england-championship' and league_name != 'england-premier-league' else ''
        file_suffix = '(2)' if league_name == 'england-premier-league' else file_suffix

        # Dynamically construct file names
        filenames = {
            'teams': f'{league_name}-teams-2023-to-2024-stats{file_suffix}.csv',
            'players': f'{league_name}-players-2023-to-2024-stats{file_suffix}.csv',
            'matches': f'{league_name}-matches-2023-to-2024-stats{file_suffix}.csv',
            'league': f'{league_name}-league-2023-to-2024-stats{file_suffix}.csv',
        }

        # Load data
        data = {key: pd.read_csv(path + filename) for key, filename in filenames.items()}

        # Example merge
        merged_df = pd.merge(data['matches'], data['teams'], left_on='home_team_name', right_on='team_name', how='left')
        # Further preprocessing here...

        all_leagues_data.append(merged_df)

    # Combine all data
    combined_df = pd.concat(all_leagues_data, ignore_index=True)
    return combined_df

path_to_files = '/content/drive/My Drive/94_betting_model/'
combined_leagues_df = load_and_preprocess_all_leagues(path_to_files)

"""DATA LOADING:"""

def load_data(file_names, path_to_files):
    data_frames = {}
    for file_name in file_names:
        df_name = file_name.split('-')[2]  # Example to get 'league', 'matches', etc.
        data_frames[df_name] = pd.read_csv(f"{path_to_files}{file_name}")
    return data_frames

file_names = [
    "europe-uefa-champions-league-league-2023-to-2024-stats.csv",
    "europe-uefa-champions-league-matches-2023-to-2024-stats.csv",
    # Add other file names
]
data_frames = load_data(file_names, path_to_files)

"""HERE IS GOOD FOR LOADING:"""

def load_and_preprocess(path, league_name):
    # Load each dataset
    team_stats = pd.read_csv(f'{path}{league_name}-teams-2023-to-2024-stats (1).csv')
    player_stats = pd.read_csv(f'{path}{league_name}-players-2023-to-2024-stats (1).csv')
    match_stats = pd.read_csv(f'{path}{league_name}-matches-2023-to-2024-stats (1).csv')
    league_stats = pd.read_csv(f'{path}{league_name}-league-2023-to-2024-stats (1).csv')

    # Preprocessing steps (you need to customize this to your data)
    # For example, if you want to merge match_stats and team_stats on team names:
    # Ensure that the team names match exactly in both dataframes, or map them to be the same.
    merged_df = pd.merge(match_stats, team_stats, left_on='home_team_name', right_on='team_name', how='left')

    # Add more preprocessing steps as needed, such as merging with player_stats and league_stats
    # and handle missing values, create new features, etc.

    # Assuming merged_df is your final merged DataFrame after all preprocessing
    return merged_df



"""Loading Data: It uses the pandas library to load various CSV files containing team, player, match, and league statistics for different football leagues. Each league has a unique file naming convention which is handled by the suffixes dictionary.

File Suffix Handling: The dictionary suffixes maps each league name to a specific suffix that is appended to the file names, allowing the correct files to be loaded for each league. Some leagues do not have a suffix, and this is handled by providing an empty string for those cases.

Data Preprocessing: After loading, the snippet merges the match statistics with team statistics twice—once for home team data and once for away team data. This is done using the pd.merge function, with appropriate suffixes to differentiate between home and away team stats.

Data Aggregation: All preprocessed data frames for each league are then appended into a list, which is finally concatenated into one comprehensive DataFrame using pd.concat. This combined DataFrame could be used for further analysis or model training.

Flexibility: The structure of the code allows for easy addition of more leagues and file suffixes, as well as additional preprocessing steps as needed.

Error Handling: The code also successfully avoids FileNotFoundError by correctly referencing the file paths and names for each league's dataset.
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Complete list of suffixes for the specified leagues
suffixes = {
    'europe-uefa-champions-league': ' (3)',
    'europe-uefa-europa-league': ' (1)',
    'europe-uefa-europa-conference-league': ' (1)',
    'england-premier-league': ' (2)',
    'spain-la-liga': ' (1)',
    'germany-bundesliga': ' (1)',
    'france-ligue-1': ' (1)',
    'portugal-liga-nos': ' (1)',
    'scotland-premiership': ' (1)',
    'italy-serie-a': ' (1)',
    'england-championship': ''  # Assuming no suffix for this league
}

def load_and_preprocess_all_leagues(path, league_names, suffixes):
    all_leagues_data = []
    for league_name in league_names:
        file_suffix = suffixes.get(league_name, '')

        # Load each dataset for the league with the appropriate file suffix
        team_stats = pd.read_csv(f'{path}{league_name}-teams-2023-to-2024-stats{file_suffix}.csv')
        player_stats = pd.read_csv(f'{path}{league_name}-players-2023-to-2024-stats{file_suffix}.csv')
        match_stats = pd.read_csv(f'{path}{league_name}-matches-2023-to-2024-stats{file_suffix}.csv')
        league_stats = pd.read_csv(f'{path}{league_name}-league-2023-to-2024-stats{file_suffix}.csv')

        # Preprocessing steps for each league
        merged_df = pd.merge(match_stats, team_stats, left_on='home_team_name', right_on='team_name', how='left')
        merged_df = pd.merge(merged_df, team_stats, left_on='away_team_name', right_on='team_name', how='left', suffixes=('_home', '_away'))

        # Add more preprocessing steps as needed

        # Append the processed DataFrame to the list
        all_leagues_data.append(merged_df)

    # Concatenate all league data into one DataFrame
    combined_df = pd.concat(all_leagues_data, ignore_index=True)
    return combined_df

# List of league names for which to load and preprocess data
league_names = [
    'europe-uefa-champions-league',
    'europe-uefa-europa-league',
    'europe-uefa-europa-conference-league',
    'england-premier-league',
    'spain-la-liga',
    'germany-bundesliga',
    'france-ligue-1',
    'portugal-liga-nos',
    'scotland-premiership',
    'italy-serie-a',
    'england-championship'
]

# Replace with your actual file path
path_to_files = '/content/drive/My Drive/94_betting_model/'

# Load and preprocess data for all leagues
combined_leagues_df = load_and_preprocess_all_leagues(path_to_files, league_names, suffixes)

from google.colab import drive
drive.mount('/content/drive')

def load_and_preprocess(path, league_name):
    file_suffix = "(1)"  # Adjust based on the league's specific naming convention

    # Load each dataset with the appropriate file suffix
    team_stats = pd.read_csv(f'{path}{league_name}-teams-2023-to-2024-stats{file_suffix}.csv')
    player_stats = pd.read_csv(f'{path}{league_name}-players-2023-to-2024-stats{file_suffix}.csv')
    match_stats = pd.read_csv(f'{path}{league_name}-matches-2023-to-2024-stats{file_suffix}.csv')
    league_stats = pd.read_csv(f'{path}{league_name}-league-2023-to-2024-stats{file_suffix}.csv')

    # Merge team stats with match stats on the home team name
    merged_df = pd.merge(match_stats, team_stats, left_on='home_team_name', right_on='team_name', how='left')

    # Then merge the result with the away team stats
    merged_df = pd.merge(merged_df, team_stats, left_on='away_team_name', right_on='team_name', how='left', suffixes=('_home', '_away'))

    # Additional preprocessing steps as required...

    # Return the preprocessed dataframe
    return merged_df

"""THIS FOR LOAD AND PREPROCESS:

Load Datasets:

The function uses the pandas library to load four different CSV files related to the specified league into separate DataFrame objects. These files contain data about teams, players, matches, and league statistics for the 2023 to 2024 season.
Preprocessing:

The function then proceeds to preprocess the loaded data. In the provided snippet, there's an example of how to merge the match_stats DataFrame with the team_stats DataFrame using a common key. The key used for merging is home_team_name from match_stats and team_name from team_stats, and the merge is done using a left join, which means all records from match_stats will be kept, and matching records from team_stats will be added where available.
The preprocessing step is crucial as it combines data from multiple sources into a single DataFrame that will be used for analysis or model training. The comment suggests that the team names should match exactly to ensure a proper merge.
Additional Preprocessing:

The comment indicates that there should be more preprocessing steps tailored to the specific needs of the data. This might include handling missing values, creating new features, or merging with additional data like player_stats and league_stats.
The final preprocessed DataFrame, merged_df, should be the result of all these preprocessing steps and is returned by the function.
Customization Needed:

The comment also notes that these preprocessing steps need to be customized according to the specific data. This means the function as provided is a template and expects further details to be filled in based on the actual data structure and analysis needs.
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Complete list of suffixes for the specified leagues
suffixes = {
    'europe-uefa-champions-league': ' (3)',
    'europe-uefa-europa-league': ' (1)',
    'europe-uefa-europa-conference-league': ' (1)',
    'england-premier-league': ' (2)',
    'spain-la-liga': ' (1)',
    'germany-bundesliga': ' (1)',
    'france-ligue-1': ' (1)',
    'portugal-liga-nos': ' (1)',
    'scotland-premiership': ' (1)',
    'italy-serie-a': ' (1)',
    'england-championship': ''  # Assuming no suffix for this league
}

def load_and_preprocess_all_leagues(path, league_names, suffixes):
    all_leagues_data = []
    for league_name in league_names:
        file_suffix = suffixes.get(league_name, '')

        # Load each dataset for the league with the appropriate file suffix
        team_stats = pd.read_csv(f'{path}{league_name}-teams-2023-to-2024-stats{file_suffix}.csv')
        player_stats = pd.read_csv(f'{path}{league_name}-players-2023-to-2024-stats{file_suffix}.csv')
        match_stats = pd.read_csv(f'{path}{league_name}-matches-2023-to-2024-stats{file_suffix}.csv')
        league_stats = pd.read_csv(f'{path}{league_name}-league-2023-to-2024-stats{file_suffix}.csv')

        # Preprocessing steps for each league
        merged_df = pd.merge(match_stats, team_stats, left_on='home_team_name', right_on='team_name', how='left')
        merged_df = pd.merge(merged_df, team_stats, left_on='away_team_name', right_on='team_name', how='left', suffixes=('_home', '_away'))

        # Add more preprocessing steps as needed
        # ...

        # Append the processed DataFrame to the list
        all_leagues_data.append(merged_df)

    # Concatenate all league data into one DataFrame
    combined_df = pd.concat(all_leagues_data, ignore_index=True)
    return combined_df

# List of league names for which to load and preprocess data
league_names = [
    'europe-uefa-champions-league',
    'europe-uefa-europa-league',
    'europe-uefa-europa-conference-league',
    'england-premier-league',
    'spain-la-liga',
    'germany-bundesliga',
    'france-ligue-1',
    'portugal-liga-nos',
    'scotland-premiership',
    'italy-serie-a',
    'england-championship'
]

# Replace with your actual file path
path_to_files = '/content/drive/My Drive/94_betting_model/'

# Load and preprocess data for all leagues
combined_leagues_df = load_and_preprocess_all_leagues(path_to_files, league_names, suffixes)

"""Function to calculate form for a given number of matches

To implement the Full-Time Result (FTR) calculation along with other necessary statistics, you can use the following steps:

Calculate Team Form: This function calculates the form of each team over the last 'n' matches.

Calculate Head-to-Head Statistics: This function calculates head-to-head statistics between two teams.

Create FTR Column: This function adds a new column to your DataFrame that indicates the full-time result of each match.

Here's the code for these steps:
"""

import pandas as pd

# Function to calculate form for a given number of matches
def calculate_form(df, team_name, n_matches=5):
    recent_matches = df[(df['home_team_name'] == team_name) | (df['away_team_name'] == team_name)]
    recent_matches = recent_matches.sort_values('date_GMT').tail(n_matches)
    points = 0
    for index, row in recent_matches.iterrows():
        if row['home_team_name'] == team_name:
            if row['home_team_goal_count'] > row['away_team_goal_count']:
                points += 3  # Home win
            elif row['home_team_goal_count'] == row['away_team_goal_count']:
                points += 1  # Draw
        else:  # Away team
            if row['away_team_goal_count'] > row['home_team_goal_count']:
                points += 3  # Away win
            elif row['away_team_goal_count'] == row['home_team_goal_count']:
                points += 1  # Draw
    return points / (3 * n_matches)  # Average points per match

# Function to calculate head-to-head statistics
def calculate_head_to_head_stats(df, team1, team2):
    matches = df[((df['home_team_name'] == team1) & (df['away_team_name'] == team2)) |
                 ((df['home_team_name'] == team2) & (df['away_team_name'] == team1))]
    home_wins = len(matches[(matches['home_team_name'] == team1) & (matches['home_team_goal_count'] > matches['away_team_goal_count'])])
    away_wins = len(matches[(matches['home_team_name'] == team2) & (matches['home_team_goal_count'] < matches['away_team_goal_count'])])
    draws = len(matches[matches['home_team_goal_count'] == matches['away_team_goal_count']])
    return {'home_wins': home_wins, 'away_wins': away_wins, 'draws': draws}

# Function to create FTR column
def create_ftr_column(df):
    df['FTR'] = df.apply(lambda row: 0 if row['home_team_goal_count'] > row['away_team_goal_count'] else 1, axis=1)

# Apply these functions to your DataFrame
# Assuming 'combined_leagues_df' is your main DataFrame
combined_leagues_df['home_team_form'] = combined_leagues_df.apply(lambda row: calculate_form(combined_leagues_df, row['home_team_name']), axis=1)
combined_leagues_df['away_team_form'] = combined_leagues_df.apply(lambda row: calculate_form(combined_leagues_df, row['away_team_name']), axis=1)
combined_leagues_df['head_to_head'] = combined_leagues_df.apply(lambda row: calculate_head_to_head_stats(combined_leagues_df, row['home_team_name'], row['away_team_name']), axis=1)
create_ftr_column(combined_leagues_df)

# Now your DataFrame will have new columns for team form, head-to-head stats, and FTR



"""Data Preprocessing

"""

import joblib

# Load the model from the file
model_path = '/content/drive/My Drive/94_betting_model/94_betting_model.pkl'  # Update with your correct path
model = joblib.load(model_path)

# Assuming 'combined_leagues_df' is your new data
# And 'selected_features' are the features used by the model
selected_features = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']

# Select the features from your new data
X_new = combined_leagues_df[selected_features]

# Make predictions
predictions = model.predict(X_new)

# Add predictions to the DataFrame
combined_leagues_df['Predicted_FTR'] = predictions

# Optionally, you can save the DataFrame with predictions to a new CSV file
combined_leagues_df.to_csv('/content/drive/My Drive/94_betting_model/predictions.csv', index=False)

"""Form Calculation: It calculates the recent form of teams based on their last n matches, helping to understand recent performance trends.

Head-to-Head Statistics: This function computes historical match outcomes between two teams, useful for understanding rivalry dynamics and historical dominance.

Feature Engineering: The create_ftr_column function creates a feature for match results, and additional columns are added for team forms and head-to-head stats, enriching your dataset for better model predictions.

Model Prediction: It loads a pre-trained model using joblib, selects relevant features from the processed data, and makes predictions on match results. These predictions are then added to your dataset.

Saving Predictions: Finally, the processed DataFrame, now with predictions, is saved to a CSV file, making it easy to analyze or share the model's output.
"""



import pandas as pd

# Load the datasets
cleaned_teams_path = 'cleaned_teams_head_to_head.csv'
cleaned_matches_path = 'cleaned_matches.csv'

# Read the data into pandas dataframes
cleaned_teams_data = pd.read_csv(cleaned_teams_path)
cleaned_matches_data = pd.read_csv(cleaned_matches_path)

# Display the first few rows to check the features
print(cleaned_teams_data.head())
print(cleaned_matches_data.head())

# Example code to check for missing values
print(cleaned_teams_data.isnull().sum())
print(cleaned_matches_data.isnull().sum())

# Fill missing values or drop rows/columns
# cleaned_teams_data.fillna(0, inplace=True)  # Example to fill missing values with 0
# cleaned_matches_data.dropna(inplace=True)   # Example to drop rows with missing values

import pandas as pd

# Replace 'path_to_your_file.csv' with the actual file path
cleaned_matches_data_df = pd.read_csv('/content/cleaned_teams_head_to_head.csv')

# Aggregating player stats by team and season
aggregation_methods = {
    'goals_per_90_overall': 'mean',
    'assists_per_90_overall': 'mean',
    'average_rating_overall': 'mean',
    'clean_sheets_overall': 'sum',
    'goals_overall': 'sum',
    'assists_overall': 'sum',
    'appearances_overall': 'sum',
    'minutes_played_overall': 'sum',
    'yellow_cards_overall': 'sum',
    'red_cards_overall': 'sum',
    'tackles_per_90_overall': 'sum',
    'xg_per_90_overall': 'sum',
    'xa_per_90_overall': 'sum',
    'npxg_per_90_overall': 'sum',
    'interceptions_per_90_overall': 'sum',
    'distance_travelled_per_90_overall': 'sum',
    'aerial_duels_won_per_90_overall': 'sum',
    'shots_on_target_per_90_overall': 'sum',
    'shot_conversion_rate_overall': 'sum',
    'key_passes_per_90_overall': 'sum',
    'through_passes_per_90_overall': 'sum',
    'chances_created_per_90_overall': 'sum',
    'dribbles_per_90_overall': 'sum',
    'dribbles_successful_percentage_overall': 'sum',
    'passes_per_90_overall': 'sum',
    'pass_completion_rate_overall': 'sum',
    'full_name': 'first',
    'age': 'first',
    'position': 'first',
    'league': 'first'
    # Note: Removed 'Current Club' and 'season' as they are already included as part of the groupby
}

aggregated_player_stats = extracted_features_df.groupby(['Current Club', 'season']).agg(aggregation_methods).reset_index()

# Assuming 'team_name' in cleaned_teams_data matches 'Current Club' in aggregated_player_stats
merged_team_player_stats = pd.merge(
    cleaned_teams_data,
    aggregated_player_stats,
    left_on=['team_name', 'season'],
    right_on=['Current Club', 'season'],
    how='left'
)

# Assuming 'home_team_name' and 'away_team_name' in cleaned_matches_data_df
final_dataset = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['common_name', 'season'],
    right_on=['team_name', 'season'],
    how='left'
)

print(cleaned_matches_data_df.columns)

# Specify the file path and name for your downloaded file
download_path = '/content/merged_team_player_stats.csv'

# Save the DataFrame to a CSV file
merged_team_player_stats.to_csv(download_path, index=False)

# Code to download the file to your local system
from google.colab import files
files.download(download_path)

cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')
print(cleaned_matches_data_df.columns)

# Replace 'path_to_your_file' with the actual path of your CSV file
cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')

# Display the first few rows of the DataFrame
print(cleaned_matches_data_df.head())

# Check column names
print(cleaned_matches_data_df.columns)

'cleaned_matches_data_df' in locals() or 'cleaned_matches_data_df' in globals()

print(cleaned_matches_data_df.columns)
print(merged_team_player_stats.columns)

cleaned_matches_data_df['season'] = pd.to_datetime(cleaned_matches_data_df['date_GMT']).dt.year

# Convert 'season' to string in both DataFrames
cleaned_matches_data_df['season'] = cleaned_matches_data_df['season'].astype(str)
merged_team_player_stats['season'] = merged_team_player_stats['season'].astype(str)

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Validate the merged data
print(final_dataset_home.head())
print(final_dataset_away.head())

# Example: Check unique team names in both DataFrames
print(cleaned_matches_data_df['home_team_name'].unique())
print(merged_team_player_stats['common_name'].unique())

# Extracting team names from the match data
team_names_match_data = set(cleaned_matches_data_df['home_team_name'].tolist() + cleaned_matches_data_df['away_team_name'].tolist())

# Extracting team names from the player stats data
team_names_player_stats = set(merged_team_player_stats['common_name'].tolist())

# Now proceed to compare the lists and find discrepancies
non_matching_names_match_data = team_names_match_data - team_names_player_stats
non_matching_names_player_stats = team_names_player_stats - team_names_match_data

# Print non-matching names for inspection
print("Non-matching in Match Data:", non_matching_names_match_data)
print("Non-matching in Player Stats:", non_matching_names_player_stats)

english_leagues = ['Premier League', 'Championship', 'FA Cup', 'League One', 'League Two']
filtered_df = original_df[original_df['league'].isin(english_leagues)]

import pandas as pd

# List all the variables currently in the environment
all_vars = globals().copy()
all_vars.update(locals())

# Filter and print only those variables which are pandas DataFrames
for var_name, var_val in all_vars.items():
    if isinstance(var_val, pd.DataFrame):
        print(f"{var_name}: {type(var_val)} with shape {var_val.shape}")

# Replace 'df_to_use' with the actual DataFrame you want to filter (e.g., extracted_features_df or combined_player_stats)
# Replace 'league_column' with the actual column name that contains league information in your DataFrame

df_to_use = extracted_features_df # or combined_player_stats
english_leagues = ['Premier League', 'Championship', 'FA Cup', 'EFL League One', 'EFL League Two']
filtered_df = df_to_use[df_to_use['league'].isin(english_leagues)]

# Display the first few rows to verify
print(filtered_df.head())



# This will display the first few rows of the DataFrame
cleaned_matches_data_df.head()

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# This will display the column names and data types
cleaned_matches_data_df.info()

# This will display the column names in the DataFrame
cleaned_matches_data_df.columns

# Commented out IPython magic to ensure Python compatibility.
# List all variables in the current session
# %who

import pandas as pd

# List all pandas DataFrames
for var in dir():
    if isinstance(eval(var), pd.DataFrame):
        print(var)

print(cleaned_matches_data_df.head())
print(cleaned_teams_data.head())
print(extracted_features_df.head())
print(combined_player_stats.head())
print(seasonal_player_stats.head())

# Example code to convert data types
# cleaned_teams_data['some_column'] = cleaned_teams_data['some_column'].astype(float)
# cleaned_matches_data['another_column'] = cleaned_matches_data['another_column'].astype(int)

import pandas as pd

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

file_paths = [
    '/content/spain-la-liga-players-2023-to-2024-stats.xlsx',
    '/content/portugal-liga-nos-players-2023-to-2024-stats.xlsx',
    '/content/italy-serie-a-players-2023-to-2024-stats.xlsx',
    '/content/germany-bundesliga-players-2023-to-2024-stats.xlsx',
    '/content/france-ligue-1-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-europa-league-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-champions-league-players-2023-to-2024-stats.xlsx',
    '/content/england-premier-league-players-2023-to-2024-stats.xlsx',
    '/content/england-fa-cup-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-two-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-one-players-2023-to-2024-stats.xlsx',
    '/content/england-championship-players-2023-to-2024-stats.xlsx',

    # Add paths for all your files
]

!pip install openpyxl
import pandas as pd

file_paths = [
    '/content/spain-la-liga-players-2023-to-2024-stats.xlsx',
    '/content/portugal-liga-nos-players-2023-to-2024-stats.xlsx',
    '/content/italy-serie-a-players-2023-to-2024-stats.xlsx',
    '/content/germany-bundesliga-players-2023-to-2024-stats.xlsx',
    '/content/france-ligue-1-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-europa-league-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-champions-league-players-2023-to-2024-stats.xlsx',
    '/content/england-premier-league-players-2023-to-2024-stats.xlsx',
    '/content/england-fa-cup-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-two-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-one-players-2023-to-2024-stats.xlsx',
    '/content/england-championship-players-2023-to-2024-stats.xlsx',

    # Add paths for all your files
]

dfs = []  # List to store individual DataFrames

for file in file_paths:
    df = pd.read_excel(file)
    dfs.append(df)

combined_player_stats = pd.concat(dfs, ignore_index=True)

print(combined_player_stats.head())

# Assuming you have a DataFrame named 'combined_player_stats' containing all the player statistics
# If you haven't already loaded the data into a DataFrame, do it before running the following code

# List of desired features
features = [
    "full_name", "age", "position", "Current Club", "league", "season",
    "minutes_played_overall", "appearances_overall", "yellow_cards_overall", "red_cards_overall",
    "goals_per_90_overall", "assists_per_90_overall", "xg_per_90_overall", "xa_per_90_overall",
    "npxg_per_90_overall", "clean_sheets_overall", "tackles_per_90_overall", "interceptions_per_90_overall",
    "distance_travelled_per_90_overall", "aerial_duels_won_per_90_overall", "goals_overall",
    "assists_overall", "shots_on_target_per_90_overall", "shot_conversion_rate_overall",
    "key_passes_per_90_overall", "through_passes_per_90_overall", "chances_created_per_90_overall",
    "dribbles_per_90_overall", "dribbles_successful_percentage_overall", "passes_per_90_overall",
    "pass_completion_rate_overall", "average_rating_overall"
]

# Extracting the features
extracted_features_df = combined_player_stats[features]

# Displaying the first few rows of the extracted features DataFrame
print(extracted_features_df.head())

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

# Define a threshold for the minimum number of non-missing values required to keep a row
threshold = len(extracted_features_df.columns) - 5  # For example, allow up to 5 missing values

# Filter out rows with missing values above the threshold
extracted_features_df = extracted_features_df.dropna(thresh=threshold)



# Check for missing values after filtering
print(extracted_features_df.isnull().sum())

# Check the size of the DataFrame after filtering
print(extracted_features_df.shape)

from google.colab import files

# Save your cleaned DataFrame to a CSV file
csv_file_path = '/content/cleaned_player_stats.csv'
extracted_features_df.to_csv(csv_file_path, index=False)

# Download the file to your local system
files.download(csv_file_path)

# Assuming 'extracted_features_df' has a 'Current Club' column that matches the team name in your match data

# If necessary, standardize team names across datasets
extracted_features_df['Current Club'] = extracted_features_df['Current Club'].str.strip().str.lower()
match_stats_df['home_team_name'] = match_stats_df['home_team_name'].str.strip().str.lower()
match_stats_df['away_team_name'] = match_stats_df['away_team_name'].str.strip().str.lower()

!pip install --upgrade pandas openpyxl

# Dropping specified columns
columns_to_drop = ['attendance', 'booked_over05_percentage_overall',
                   'booked_over05_percentage_percentile_overall',
                   'shirt_number', 'annual_salary_gbp', 'annual_salary_usd']

combined_player_stats.drop(columns=columns_to_drop, inplace=True, errors='ignore')
cleaned_matches_data_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

# Checking the DataFrame after dropping the columns
print(combined_player_stats.head())
print(cleaned_matches_data_df.head())

import pandas as pd

# Load the datasets
cleaned_teams_path = 'cleaned_teams_head_to_head.csv'
cleaned_matches_path = 'cleaned_matches.csv'

# Read the data into pandas dataframes
cleaned_teams_data = pd.read_csv(cleaned_teams_path)
cleaned_matches_data = pd.read_csv(cleaned_matches_path)

# Display the first few rows to check the features
print(cleaned_teams_data.head())
print(cleaned_matches_data.head())

# Example code to check for missing values
print(cleaned_teams_data.isnull().sum())
print(cleaned_matches_data.isnull().sum())

# Fill missing values or drop rows/columns
# cleaned_teams_data.fillna(0, inplace=True)  # Example to fill missing values with 0
# cleaned_matches_data.dropna(inplace=True)   # Example to drop rows with missing values

import pandas as pd

# Replace 'path_to_your_file.csv' with the actual file path
cleaned_matches_data_df = pd.read_csv('/content/cleaned_teams_head_to_head.csv')

# Aggregating player stats by team and season
aggregation_methods = {
    'goals_per_90_overall': 'mean',
    'assists_per_90_overall': 'mean',
    'average_rating_overall': 'mean',
    'clean_sheets_overall': 'sum',
    'goals_overall': 'sum',
    'assists_overall': 'sum',
    'appearances_overall': 'sum',
    'minutes_played_overall': 'sum',
    'yellow_cards_overall': 'sum',
    'red_cards_overall': 'sum',
    'tackles_per_90_overall': 'sum',
    'xg_per_90_overall': 'sum',
    'xa_per_90_overall': 'sum',
    'npxg_per_90_overall': 'sum',
    'interceptions_per_90_overall': 'sum',
    'distance_travelled_per_90_overall': 'sum',
    'aerial_duels_won_per_90_overall': 'sum',
    'shots_on_target_per_90_overall': 'sum',
    'shot_conversion_rate_overall': 'sum',
    'key_passes_per_90_overall': 'sum',
    'through_passes_per_90_overall': 'sum',
    'chances_created_per_90_overall': 'sum',
    'dribbles_per_90_overall': 'sum',
    'dribbles_successful_percentage_overall': 'sum',
    'passes_per_90_overall': 'sum',
    'pass_completion_rate_overall': 'sum',
    'full_name': 'first',
    'age': 'first',
    'position': 'first',
    'league': 'first'
    # Note: Removed 'Current Club' and 'season' as they are already included as part of the groupby
}

aggregated_player_stats = extracted_features_df.groupby(['Current Club', 'season']).agg(aggregation_methods).reset_index()

# Assuming 'team_name' in cleaned_teams_data matches 'Current Club' in aggregated_player_stats
merged_team_player_stats = pd.merge(
    cleaned_teams_data,
    aggregated_player_stats,
    left_on=['team_name', 'season'],
    right_on=['Current Club', 'season'],
    how='left'
)

# Assuming 'home_team_name' and 'away_team_name' in cleaned_matches_data_df
final_dataset = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['common_name', 'season'],
    right_on=['team_name', 'season'],
    how='left'
)

print(cleaned_matches_data_df.columns)

# Specify the file path and name for your downloaded file
download_path = '/content/merged_team_player_stats.csv'

# Save the DataFrame to a CSV file
merged_team_player_stats.to_csv(download_path, index=False)

# Code to download the file to your local system
from google.colab import files
files.download(download_path)

cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')
print(cleaned_matches_data_df.columns)

# Replace 'path_to_your_file' with the actual path of your CSV file
cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')

# Display the first few rows of the DataFrame
print(cleaned_matches_data_df.head())

# Check column names
print(cleaned_matches_data_df.columns)

'cleaned_matches_data_df' in locals() or 'cleaned_matches_data_df' in globals()

print(cleaned_matches_data_df.columns)
print(merged_team_player_stats.columns)

cleaned_matches_data_df['season'] = pd.to_datetime(cleaned_matches_data_df['date_GMT']).dt.year

# Convert 'season' to string in both DataFrames
cleaned_matches_data_df['season'] = cleaned_matches_data_df['season'].astype(str)
merged_team_player_stats['season'] = merged_team_player_stats['season'].astype(str)

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Validate the merged data
print(final_dataset_home.head())
print(final_dataset_away.head())

# Example: Check unique team names in both DataFrames
print(cleaned_matches_data_df['home_team_name'].unique())
print(merged_team_player_stats['common_name'].unique())

# Extracting team names from the match data
team_names_match_data = set(cleaned_matches_data_df['home_team_name'].tolist() + cleaned_matches_data_df['away_team_name'].tolist())

# Extracting team names from the player stats data
team_names_player_stats = set(merged_team_player_stats['common_name'].tolist())

# Now proceed to compare the lists and find discrepancies
non_matching_names_match_data = team_names_match_data - team_names_player_stats
non_matching_names_player_stats = team_names_player_stats - team_names_match_data

# Print non-matching names for inspection
print("Non-matching in Match Data:", non_matching_names_match_data)
print("Non-matching in Player Stats:", non_matching_names_player_stats)

english_leagues = ['Premier League', 'Championship', 'FA Cup', 'League One', 'League Two']
filtered_df = original_df[original_df['league'].isin(english_leagues)]

import pandas as pd

# List all the variables currently in the environment
all_vars = globals().copy()
all_vars.update(locals())

# Filter and print only those variables which are pandas DataFrames
for var_name, var_val in all_vars.items():
    if isinstance(var_val, pd.DataFrame):
        print(f"{var_name}: {type(var_val)} with shape {var_val.shape}")

# Replace 'df_to_use' with the actual DataFrame you want to filter (e.g., extracted_features_df or combined_player_stats)
# Replace 'league_column' with the actual column name that contains league information in your DataFrame

df_to_use = extracted_features_df # or combined_player_stats
english_leagues = ['Premier League', 'Championship', 'FA Cup', 'EFL League One', 'EFL League Two']
filtered_df = df_to_use[df_to_use['league'].isin(english_leagues)]

# Display the first few rows to verify
print(filtered_df.head())



# This will display the first few rows of the DataFrame
cleaned_matches_data_df.head()

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# This will display the column names and data types
cleaned_matches_data_df.info()

# This will display the column names in the DataFrame
cleaned_matches_data_df.columns

# Commented out IPython magic to ensure Python compatibility.
# List all variables in the current session
# %who

import pandas as pd

# List all pandas DataFrames
for var in dir():
    if isinstance(eval(var), pd.DataFrame):
        print(var)

print(cleaned_matches_data_df.head())
print(cleaned_teams_data.head())
print(extracted_features_df.head())
print(combined_player_stats.head())
print(seasonal_player_stats.head())

# Example code to convert data types
# cleaned_teams_data['some_column'] = cleaned_teams_data['some_column'].astype(float)
# cleaned_matches_data['another_column'] = cleaned_matches_data['another_column'].astype(int)

import pandas as pd

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

file_paths = [


    # Add paths for all your files
]

!pip install openpyxl
import pandas as pd

file_paths = [


    # Add paths for all your files
]

dfs = []  # List to store individual DataFrames

for file in file_paths:
    df = pd.read_excel(file)
    dfs.append(df)

combined_player_stats = pd.concat(dfs, ignore_index=True)

print(combined_player_stats.head())

# Assuming you have a DataFrame named 'combined_player_stats' containing all the player statistics
# If you haven't already loaded the data into a DataFrame, do it before running the following code

# List of desired features
features = [
    "full_name", "age", "position", "Current Club", "league", "season",
    "minutes_played_overall", "appearances_overall", "yellow_cards_overall", "red_cards_overall",
    "goals_per_90_overall", "assists_per_90_overall", "xg_per_90_overall", "xa_per_90_overall",
    "npxg_per_90_overall", "clean_sheets_overall", "tackles_per_90_overall", "interceptions_per_90_overall",
    "distance_travelled_per_90_overall", "aerial_duels_won_per_90_overall", "goals_overall",
    "assists_overall", "shots_on_target_per_90_overall", "shot_conversion_rate_overall",
    "key_passes_per_90_overall", "through_passes_per_90_overall", "chances_created_per_90_overall",
    "dribbles_per_90_overall", "dribbles_successful_percentage_overall", "passes_per_90_overall",
    "pass_completion_rate_overall", "average_rating_overall"
]

# Extracting the features
extracted_features_df = combined_player_stats[features]

# Displaying the first few rows of the extracted features DataFrame
print(extracted_features_df.head())

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

# Define a threshold for the minimum number of non-missing values required to keep a row
threshold = len(extracted_features_df.columns) - 5  # For example, allow up to 5 missing values

# Filter out rows with missing values above the threshold
extracted_features_df = extracted_features_df.dropna(thresh=threshold)



# Check for missing values after filtering
print(extracted_features_df.isnull().sum())

# Check the size of the DataFrame after filtering
print(extracted_features_df.shape)

from google.colab import files

# Save your cleaned DataFrame to a CSV file
csv_file_path = '/content/cleaned_player_stats.csv'
extracted_features_df.to_csv(csv_file_path, index=False)

# Download the file to your local system
files.download(csv_file_path)

# Assuming 'extracted_features_df' has a 'Current Club' column that matches the team name in your match data

# If necessary, standardize team names across datasets
extracted_features_df['Current Club'] = extracted_features_df['Current Club'].str.strip().str.lower()
match_stats_df['home_team_name'] = match_stats_df['home_team_name'].str.strip().str.lower()
match_stats_df['away_team_name'] = match_stats_df['away_team_name'].str.strip().str.lower()

# Standardize team names
teams_df['team_name'] = teams_df['team_name'].str.lower().str.strip()
matches_df['home_team_name'] = matches_df['home_team_name'].str.lower().str.strip()
matches_df['away_team_name'] = matches_df['away_team_name'].str.lower().str.strip()

# Merge head-to-head data with matches
# Assuming 'home_team_name' and 'away_team_name' are the keys
matches_enriched_df = pd.merge(matches_df, head_to_head_df, on=['home_team_name', 'away_team_name'], how='left')

import pandas as pd

# Paths to your datasets
teams_path = '/content/drive/My Drive/94_betting_model/cleaned_teams.csv'
matches_path = '/content/drive/My Drive/94_betting_model/cleaned_matches.csv'
head_to_head_path = '/content/drive/My Drive/94_betting_model/head_to_head.csv'

# Load datasets
teams_df = pd.read_csv(teams_path)
matches_df = pd.read_csv(matches_path)
head_to_head_df = pd.read_csv(head_to_head_path)

# Preliminary cleaning steps (e.g., handling missing values)
teams_df.fillna(0, inplace=True)
matches_df.dropna(inplace=True)

import pandas as pd

# Load the datasets
cleaned_teams_path = 'cleaned_teams_head_to_head.csv'
cleaned_matches_path = 'cleaned_matches.csv'

# Read the data into pandas dataframes
cleaned_teams_data = pd.read_csv(cleaned_teams_path)
cleaned_matches_data = pd.read_csv(cleaned_matches_path)

# Display the first few rows to check the features
print(cleaned_teams_data.head())
print(cleaned_matches_data.head())

# Example code to check for missing values
print(cleaned_teams_data.isnull().sum())
print(cleaned_matches_data.isnull().sum())

# Fill missing values or drop rows/columns
# cleaned_teams_data.fillna(0, inplace=True)  # Example to fill missing values with 0
# cleaned_matches_data.dropna(inplace=True)   # Example to drop rows with missing values

import pandas as pd

# Replace 'path_to_your_file.csv' with the actual file path
cleaned_matches_data_df = pd.read_csv('/content/drive/My Drive/94_betting_model/cleaned_teams_head_to_head.csv')

# Aggregating player stats by team and season
aggregation_methods = {
    'goals_per_90_overall': 'mean',
    'assists_per_90_overall': 'mean',
    'average_rating_overall': 'mean',
    'clean_sheets_overall': 'sum',
    'goals_overall': 'sum',
    'assists_overall': 'sum',
    'appearances_overall': 'sum',
    'minutes_played_overall': 'sum',
    'yellow_cards_overall': 'sum',
    'red_cards_overall': 'sum',
    'tackles_per_90_overall': 'sum',
    'xg_per_90_overall': 'sum',
    'xa_per_90_overall': 'sum',
    'npxg_per_90_overall': 'sum',
    'interceptions_per_90_overall': 'sum',
    'distance_travelled_per_90_overall': 'sum',
    'aerial_duels_won_per_90_overall': 'sum',
    'shots_on_target_per_90_overall': 'sum',
    'shot_conversion_rate_overall': 'sum',
    'key_passes_per_90_overall': 'sum',
    'through_passes_per_90_overall': 'sum',
    'chances_created_per_90_overall': 'sum',
    'dribbles_per_90_overall': 'sum',
    'dribbles_successful_percentage_overall': 'sum',
    'passes_per_90_overall': 'sum',
    'pass_completion_rate_overall': 'sum',
    'full_name': 'first',
    'age': 'first',
    'position': 'first',
    'league': 'first'
    # Note: Removed 'Current Club' and 'season' as they are already included as part of the groupby
}

aggregated_player_stats = extracted_features_df.groupby(['Current Club', 'season']).agg(aggregation_methods).reset_index()

# Assuming 'team_name' in cleaned_teams_data matches 'Current Club' in aggregated_player_stats
merged_team_player_stats = pd.merge(
    cleaned_teams_data,
    aggregated_player_stats,
    left_on=['team_name', 'season'],
    right_on=['Current Club', 'season'],
    how='left'
)

# Assuming 'home_team_name' and 'away_team_name' in cleaned_matches_data_df
final_dataset = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['common_name', 'season'],
    right_on=['team_name', 'season'],
    how='left'
)

print(cleaned_matches_data_df.columns)

# Specify the file path and name for your downloaded file
download_path = '/content/merged_team_player_stats.csv'

# Save the DataFrame to a CSV file
merged_team_player_stats.to_csv(download_path, index=False)

# Code to download the file to your local system
from google.colab import files
files.download(download_path)

cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')
print(cleaned_matches_data_df.columns)

# Replace 'path_to_your_file' with the actual path of your CSV file
cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches (2).csv')

# Display the first few rows of the DataFrame
print(cleaned_matches_data_df.head())

# Check column names
print(cleaned_matches_data_df.columns)

'cleaned_matches_data_df' in locals() or 'cleaned_matches_data_df' in globals()

print(cleaned_matches_data_df.columns)
print(merged_team_player_stats.columns)

cleaned_matches_data_df['season'] = pd.to_datetime(cleaned_matches_data_df['date_GMT']).dt.year

# Convert 'season' to string in both DataFrames
cleaned_matches_data_df['season'] = cleaned_matches_data_df['season'].astype(str)
merged_team_player_stats['season'] = merged_team_player_stats['season'].astype(str)

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Validate the merged data
print(final_dataset_home.head())
print(final_dataset_away.head())

# Example: Check unique team names in both DataFrames
print(cleaned_matches_data_df['home_team_name'].unique())
print(merged_team_player_stats['common_name'].unique())

# Extracting team names from the match data
team_names_match_data = set(cleaned_matches_data_df['home_team_name'].tolist() + cleaned_matches_data_df['away_team_name'].tolist())

# Extracting team names from the player stats data
team_names_player_stats = set(merged_team_player_stats['common_name'].tolist())

# Now proceed to compare the lists and find discrepancies
non_matching_names_match_data = team_names_match_data - team_names_player_stats
non_matching_names_player_stats = team_names_player_stats - team_names_match_data

# Print non-matching names for inspection
print("Non-matching in Match Data:", non_matching_names_match_data)
print("Non-matching in Player Stats:", non_matching_names_player_stats)

english_leagues = ['Premier League', 'Championship', 'FA Cup', 'League One', 'League Two']
filtered_df = original_df[original_df['league'].isin(english_leagues)]

import pandas as pd

# List all the variables currently in the environment
all_vars = globals().copy()
all_vars.update(locals())

# Filter and print only those variables which are pandas DataFrames
for var_name, var_val in all_vars.items():
    if isinstance(var_val, pd.DataFrame):
        print(f"{var_name}: {type(var_val)} with shape {var_val.shape}")

# Replace 'df_to_use' with the actual DataFrame you want to filter (e.g., extracted_features_df or combined_player_stats)
# Replace 'league_column' with the actual column name that contains league information in your DataFrame

df_to_use = extracted_features_df # or combined_player_stats
english_leagues = ['Premier League', 'Championship', 'FA Cup', 'EFL League One', 'EFL League Two']
filtered_df = df_to_use[df_to_use['league'].isin(english_leagues)]

# Display the first few rows to verify
print(filtered_df.head())



# This will display the first few rows of the DataFrame
cleaned_matches_data_df.head()

# Merge considering 'common_name' as the home team
final_dataset_home = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['home_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# Merge considering 'common_name' as the away team
final_dataset_away = pd.merge(
    cleaned_matches_data_df,
    merged_team_player_stats,
    left_on=['away_team_name', 'season'],
    right_on=['common_name', 'season'],
    how='left'
)

# This will display the column names and data types
cleaned_matches_data_df.info()

# This will display the column names in the DataFrame
cleaned_matches_data_df.columns

# Commented out IPython magic to ensure Python compatibility.
# List all variables in the current session
# %who

import pandas as pd

# List all pandas DataFrames
for var in dir():
    if isinstance(eval(var), pd.DataFrame):
        print(var)

print(cleaned_matches_data_df.head())
print(cleaned_teams_data.head())
print(extracted_features_df.head())
print(combined_player_stats.head())
print(seasonal_player_stats.head())

# Example code to convert data types
# cleaned_teams_data['some_column'] = cleaned_teams_data['some_column'].astype(float)
# cleaned_matches_data['another_column'] = cleaned_matches_data['another_column'].astype(int)

import pandas as pd

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

file_paths = [
    '/content/spain-la-liga-players-2023-to-2024-stats.xlsx',
    '/content/portugal-liga-nos-players-2023-to-2024-stats.xlsx',
    '/content/italy-serie-a-players-2023-to-2024-stats.xlsx',
    '/content/germany-bundesliga-players-2023-to-2024-stats.xlsx',
    '/content/france-ligue-1-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-europa-league-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-champions-league-players-2023-to-2024-stats.xlsx',
    '/content/england-premier-league-players-2023-to-2024-stats.xlsx',
    '/content/england-fa-cup-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-two-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-one-players-2023-to-2024-stats.xlsx',
    '/content/england-championship-players-2023-to-2024-stats.xlsx',

    # Add paths for all your files
]

!pip install openpyxl
import pandas as pd

file_paths = [
    '/content/spain-la-liga-players-2023-to-2024-stats.xlsx',
    '/content/portugal-liga-nos-players-2023-to-2024-stats.xlsx',
    '/content/italy-serie-a-players-2023-to-2024-stats.xlsx',
    '/content/germany-bundesliga-players-2023-to-2024-stats.xlsx',
    '/content/france-ligue-1-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-europa-league-players-2023-to-2024-stats.xlsx',
    '/content/europe-uefa-champions-league-players-2023-to-2024-stats.xlsx',
    '/content/england-premier-league-players-2023-to-2024-stats.xlsx',
    '/content/england-fa-cup-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-two-players-2023-to-2024-stats.xlsx',
    '/content/england-efl-league-one-players-2023-to-2024-stats.xlsx',
    '/content/england-championship-players-2023-to-2024-stats.xlsx',

    # Add paths for all your files
]

dfs = []  # List to store individual DataFrames

for file in file_paths:
    df = pd.read_excel(file)
    dfs.append(df)

combined_player_stats = pd.concat(dfs, ignore_index=True)

print(combined_player_stats.head())

# Assuming you have a DataFrame named 'combined_player_stats' containing all the player statistics
# If you haven't already loaded the data into a DataFrame, do it before running the following code

# List of desired features
features = [
    "full_name", "age", "position", "Current Club", "league", "season",
    "minutes_played_overall", "appearances_overall", "yellow_cards_overall", "red_cards_overall",
    "goals_per_90_overall", "assists_per_90_overall", "xg_per_90_overall", "xa_per_90_overall",
    "npxg_per_90_overall", "clean_sheets_overall", "tackles_per_90_overall", "interceptions_per_90_overall",
    "distance_travelled_per_90_overall", "aerial_duels_won_per_90_overall", "goals_overall",
    "assists_overall", "shots_on_target_per_90_overall", "shot_conversion_rate_overall",
    "key_passes_per_90_overall", "through_passes_per_90_overall", "chances_created_per_90_overall",
    "dribbles_per_90_overall", "dribbles_successful_percentage_overall", "passes_per_90_overall",
    "pass_completion_rate_overall", "average_rating_overall"
]

# Extracting the features
extracted_features_df = combined_player_stats[features]

# Displaying the first few rows of the extracted features DataFrame
print(extracted_features_df.head())

# Check for missing values
missing_values = extracted_features_df.isnull().sum()
print(missing_values)

# Define a threshold for the minimum number of non-missing values required to keep a row
threshold = len(extracted_features_df.columns) - 5  # For example, allow up to 5 missing values

# Filter out rows with missing values above the threshold
extracted_features_df = extracted_features_df.dropna(thresh=threshold)



# Check for missing values after filtering
print(extracted_features_df.isnull().sum())

# Check the size of the DataFrame after filtering
print(extracted_features_df.shape)

from google.colab import files

# Save your cleaned DataFrame to a CSV file
csv_file_path = '/content/cleaned_player_stats.csv'
extracted_features_df.to_csv(csv_file_path, index=False)

# Download the file to your local system
files.download(csv_file_path)

# Assuming 'extracted_features_df' has a 'Current Club' column that matches the team name in your match data

# If necessary, standardize team names across datasets
extracted_features_df['Current Club'] = extracted_features_df['Current Club'].str.strip().str.lower()
match_stats_df['home_team_name'] = match_stats_df['home_team_name'].str.strip().str.lower()
match_stats_df['away_team_name'] = match_stats_df['away_team_name'].str.strip().str.lower()

!pip install --upgrade pandas openpyxl

X = data_frames['matches'][feature_columns]  # Define your features columns
y = data_frames['matches']['target_column']  # Define your target column

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = XGBClassifier()
model.fit(X_train, y_train)

# Evaluate model here, e.g., using accuracy_score or classification_report from sklearn.metrics

"""## Model Training and Prediction"""

Model Training and Prediction
(Code for model training and prediction will be added here)



import pandas as pd

# Load your predictions DataFrame
# Replace 'your_predictions_file_path' with the path to your CSV file containing predictions
predictions_df = pd.read_csv('/content/drive/My Drive/94_betting_model/predictions.csv')

# Analyze Prediction Patterns
home_wins = predictions_df[predictions_df['FTR'] == 0].shape[0]  # Assuming 0 represents home wins
draws = predictions_df[predictions_df['FTR'] == 1].shape[0]      # Assuming 1 represents draws
away_wins = predictions_df[predictions_df['FTR'] == 2].shape[0]  # Assuming 2 represents away wins

print(f"Home Wins: {home_wins}, Draws: {draws}, Away Wins: {away_wins}")

# Exporting Data for Records
predictions_df.to_csv('your_predictions.csv', index=False)

# Note: Since actual outcomes are not available, accuracy cannot be computed at this stage.

# Print the filtered DataFrame
print(matches_on_dates)

# Save the filtered DataFrame to a CSV file
csv_filename = '/content/drive/My Drive/94_betting_model/matches_on_specific_dates.csv'
matches_on_dates.to_csv(csv_filename, index=False)

print(f"Filtered matches saved to {csv_filename}")

# Check if the DataFrame contains the required dates
print("Dates in the dataset:", combined_leagues_df['date_GMT'].unique())

# Check if the feature columns are available in the DataFrame
print("Columns in the DataFrame:", combined_leagues_df.columns)



# Check if X is empty
if X.empty:
    print("The feature DataFrame 'X' is empty.")
else:
    # Print the shape and column names of X
    print("Shape of X:", X.shape)
    print("Columns in X:", X.columns.tolist())

    # Optionally, check the first few rows of X to verify data
    print(X.head())

# Check the columns of both dataframes
print("Columns in cleaned_matches_data_df:", cleaned_matches_data_df.columns)
print("Columns in combined_player_stats:", combined_player_stats.columns)



"""## Model Training and Prediction"""

Model Training and Prediction
(Code for model training and prediction will be added here)



import joblib
import pandas as pd

# Load the model from the file
model_path = '/content/drive/My Drive/94_betting_model/94_betting_model.pkl'  # Update with your correct path
model = joblib.load(model_path)

# Assuming 'combined_leagues_df' is your new data
# And 'selected_features' are the features used by the model
selected_features = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']

# Select the features from your new data
X_new = combined_leagues_df[selected_features]

# Check if X_new is empty
if X_new.empty:
    print("The feature DataFrame is empty. Check your data selection process.")
else:
    # Make predictions
    predictions = model.predict(X_new)

    # Add predictions to the DataFrame
    combined_leagues_df['Predicted_FTR'] = predictions

    # Optionally, save the DataFrame with predictions to a new CSV file
    combined_leagues_df.to_csv('/content/drive/My Drive/94_betting_model/predictions3.csv', index=False)

    print("Predictions made and saved successfully.")

import pandas as pd

# Assuming 'combined_leagues_df' is your DataFrame with the features and 'model' is your trained model

# Filter for the dates you're interested in (e.g., Nov 28 and Nov 29, 2023)
dates_of_interest = ['2023-11-28', '2023-11-29']
filtered_df = combined_leagues_df[combined_leagues_df['date_GMT'].isin(dates_of_interest)]

# Extract features for prediction
feature_columns = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
X = filtered_df[feature_columns]

# Make predictions and get probabilities
predicted_results = model.predict(X)
predicted_probabilities = model.predict_proba(X)

# Add predictions and probabilities back to the DataFrame
filtered_df['predicted_result'] = predicted_results
filtered_df['probability_home_win'] = predicted_probabilities[:, 0]  # Probability of Home Win
filtered_df['probability_draw'] = predicted_probabilities[:, 1]      # Probability of Draw
filtered_df['probability_away_win'] = predicted_probabilities[:, 2]  # Probability of Away Win

# Select columns to display
columns_to_display = ['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'probability_home_win', 'probability_draw', 'probability_away_win']
filtered_predictions = filtered_df[columns_to_display]

print(filtered_predictions)

# Assuming 'predictions_df' is your DataFrame with predictions and other relevant columns
predictions_df.to_csv('/content/drive/My Drive/94_betting_model/predictions_2.csv', index=False)

!pip install shap

# Convert 'date_GMT' to datetime
combined_leagues_df['date_GMT'] = pd.to_datetime(combined_leagues_df['date_GMT'])

# Check the range of dates in the dataset
print("Earliest date in the dataset:", combined_leagues_df['date_GMT'].min())
print("Latest date in the dataset:", combined_leagues_df['date_GMT'].max())

# Filter for November 30, 2023
nov_30_predictions = combined_leagues_df[combined_leagues_df['date_GMT'] == pd.Timestamp('2023-11-30')]

# Check if there are any matches on this date
if nov_30_predictions.empty:
    print("No matches found for November 30, 2023.")
else:
    # Extract features and make predictions
    X_nov_30 = nov_30_predictions[feature_columns]
    nov_30_predictions['predicted_result'] = model.predict(X_nov_30)
    nov_30_predictions['predicted_probability'] = model.predict_proba(X_nov_30)[:,1]

    # Display the predictions
    print(nov_30_predictions[['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'predicted_probability']])

"""rhis next code snippet is for searching for dates: chnage this so its easier to update / automate in the future to update ranges based on specific ranges set

"""

# Filter for November 28 and 29, 2023
nov_28_predictions = combined_leagues_df[combined_leagues_df['date_GMT'] == pd.Timestamp('2023-11-28')]
nov_29_predictions = combined_leagues_df[combined_leagues_df['date_GMT'] == pd.Timestamp('2023-11-29')]

# Check if there are any matches on these dates
if nov_28_predictions.empty and nov_29_predictions.empty:
    print("No matches found for November 28 and 29, 2023.")
else:
    # If there are matches on November 28
    if not nov_28_predictions.empty:
        X_nov_28 = nov_28_predictions[feature_columns]
        nov_28_predictions['predicted_result'] = model.predict(X_nov_28)
        nov_28_predictions['predicted_probability'] = model.predict_proba(X_nov_28)[:,1]
        print("Predictions for November 28, 2023:")
        print(nov_28_predictions[['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'predicted_probability']])

    # If there are matches on November 29
    if not nov_29_predictions.empty:
        X_nov_29 = nov_29_predictions[feature_columns]
        nov_29_predictions['predicted_result'] = model.predict(X_nov_29)
        nov_29_predictions['predicted_probability'] = model.predict_proba(X_nov_29)[:,1]
        print("Predictions for November 29, 2023:")
        print(nov_29_predictions[['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'predicted_probability']])

# Filter for November 30, 2023, at specific times
nov_30_545pm_predictions = combined_leagues_df[combined_leagues_df['date_GMT'] == pd.Timestamp('2023-11-30 17:45:00')]
nov_30_800pm_predictions = combined_leagues_df[combined_leagues_df['date_GMT'] == pd.Timestamp('2023-11-30 20:00:00')]

# Check if there are any matches at these times
if nov_30_545pm_predictions.empty and nov_30_800pm_predictions.empty:
    print("No matches found for November 30, 2023, at 5:45 pm and 8:00 pm.")
else:
    # If there are matches at 5:45 pm
    if not nov_30_545pm_predictions.empty:
        X_nov_30_545pm = nov_30_545pm_predictions[feature_columns]
        nov_30_545pm_predictions['predicted_result'] = model.predict(X_nov_30_545pm)
        nov_30_545pm_predictions['predicted_probability'] = model.predict_proba(X_nov_30_545pm)[:,1]
        print("Predictions for November 30, 2023, at 5:45 pm:")
        print(nov_30_545pm_predictions[['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'predicted_probability']])

    # If there are matches at 8:00 pm
    if not nov_30_800pm_predictions.empty:
        X_nov_30_800pm = nov_30_800pm_predictions[feature_columns]
        nov_30_800pm_predictions['predicted_result'] = model.predict(X_nov_30_800pm)
        nov_30_800pm_predictions['predicted_probability'] = model.predict_proba(X_nov_30_800pm)[:,1]
        print("Predictions for November 30, 2023, at 8:00 pm:")
        print(nov_30_800pm_predictions[['date_GMT', 'home_team_name', 'away_team_name', 'predicted_result', 'predicted_probability']])

# Calculate SHAP values using KernelExplainer
explainer = shap.KernelExplainer(model.predict_proba, X)
shap_values = explainer.shap_values(X)

# Plot SHAP values for the first prediction
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1][0], X.iloc[0])

def calculate_ftr(home_goals, away_goals):
    """
    Calculate the Full Time Result (FTR).
    Home win (0), draw (1), away win (2).
    """
    if home_goals > away_goals:
        return 0  # Home win
    elif home_goals == away_goals:
        return 1  # Draw
    else:
        return 2  # Away win

# Apply the FTR calculation to each row in the DataFrame
combined_leagues_df['FTR'] = combined_leagues_df.apply(
    lambda row: calculate_ftr(row['home_team_goal_count'], row['away_team_goal_count']), axis=1
)

# Selecting the necessary features
selected_features = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count',
                     'away_team_goal_count', 'average_corners_per_match_pre_match',
                     'average_cards_per_match_pre_match', 'odds_ft_home_team_win',
                     'odds_ft_draw', 'odds_ft_away_team_win']

# Assuming 'merged_df' is your preprocessed DataFrame
X = combined_leagues_df[selected_features]

# Assuming 'FTR' is your target variable (full-time result)
y = combined_leagues_df['FTR']









import pandas as pd

# Assuming you have mounted your Google Drive and the path to your files is set correctly
path_to_files = '/content/drive/My Drive/'  # Update this with the correct path

# Load each file into a DataFrame
league_stats_df = pd.read_csv(path_to_files + "europe-uefa-europa-league-league-2023-to-2024-stats.csv")
matches_df = pd.read_csv(path_to_files + "europe-uefa-europa-league-matches-2023-to-2024-stats.csv")
players_df = pd.read_csv(path_to_files + "europe-uefa-europa-league-players-2023-to-2024-stats.csv")
teams_df = pd.read_csv(path_to_files + "europe-uefa-europa-league-teams-2023-to-2024-stats.csv")

# Display the first few rows of each DataFrame to verify
print(league_stats_df.head())
print(matches_df.head())
print(players_df.head())
print(teams_df.head())

# Commented out IPython magic to ensure Python compatibility.
# %whos

matches_df.head()

# Checking for missing values
print("Missing values in matches data:")
print(cleaned_matches_data_df.isnull().sum())

print("\nMissing values in player stats data:")
print(combined_player_stats.isnull().sum())

# Handling missing values - Example: fill with a default value or drop
# cleaned_matches_data_df.fillna(0, inplace=True)
# combined_player_stats.dropna(inplace=True)



"""print(league_stats_df.head()) print(matches_df.head()) print(players_df.head()) print(teams_df.head"""

import pandas as pd

# Assuming the file names are consistent with your upload
cleaned_matches_data_df = pd.read_csv(path_to_files + 'europe-uefa-champions-league-matches-2023-to-2024-stats.csv')
combined_player_stats = pd.read_csv(path_to_files + 'europe-uefa-champions-league-players-2023-to-2024-stats.csv')

import pandas as pd

# Assuming the file names are consistent with your upload
cleaned_matches_data_df = pd.read_csv(path_to_files + 'europe-uefa-europa-league-matches-2023-to-2024-stats.csv')
combined_player_stats = pd.read_csv(path_to_files + 'europe-uefa-europa-league-players-2023-to-2024-stats.csv')

print(cleaned_matches_data_df.head())
print(combined_player_stats.head())

# Checking for missing values
print("Missing values in matches data:")
print(cleaned_matches_data_df.isnull().sum())

print("\nMissing values in player stats data:")
print(combined_player_stats.isnull().sum())

# Handling missing values - Example: fill with a default value or drop
# cleaned_matches_data_df.fillna(0, inplace=True)
# combined_player_stats.dropna(inplace=True)

# Detecting outliers - Example with IQR
Q1 = combined_player_stats.quantile(0.25)
Q3 = combined_player_stats.quantile(0.75)
IQR = Q3 - Q1

# Filtering out the outliers
filtered_player_stats = combined_player_stats[~((combined_player_stats < (Q1 - 1.5 * IQR)) |(combined_player_stats > (Q3 + 1.5 * IQR))).any(axis=1)]

columns_to_remove = ['booked_over05_percentage_overall',
                     'booked_over05_percentage_percentile_overall',
                     'shirt_number',
                     'annual_salary_gbp',
                     'annual_salary_usd']
combined_player_stats.drop(columns=columns_to_remove, inplace=True, errors='ignore')

# Install unidecode
!pip install unidecode

# Import unidecode
import unidecode

# Apply unidecode to standardize player names
combined_player_stats['full_name'] = combined_player_stats['full_name'].apply(unidecode.unidecode)

# Check the changes
print(combined_player_stats.head())

# Dropping specified columns
columns_to_drop = ['attendance', 'booked_over05_percentage_overall',
                   'booked_over05_percentage_percentile_overall',
                   'shirt_number', 'annual_salary_gbp', 'annual_salary_usd']

combined_player_stats.drop(columns=columns_to_drop, inplace=True, errors='ignore')
cleaned_matches_data_df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

# Checking the DataFrame after dropping the columns
print(combined_player_stats.head())
print(cleaned_matches_data_df.head())

# Check the columns of both dataframes
print("Columns in cleaned_matches_data_df:", cleaned_matches_data_df.columns)
print("Columns in combined_player_stats:", combined_player_stats.columns)

# Merging player stats with home team stats
home_merged = pd.merge(cleaned_matches_data_df,
                       combined_player_stats,
                       left_on=['home_team_name', 'season'],
                       right_on=['Current Club', 'season'])

# Merging player stats with away team stats
away_merged = pd.merge(cleaned_matches_data_df,
                       combined_player_stats,
                       left_on=['away_team_name', 'season'],
                       right_on=['Current Club', 'season'])



print("Columns in cleaned_matches_data_df:", cleaned_matches_data_df.columns.tolist())
print("Columns in combined_player_stats:", combined_player_stats.columns.tolist())

# Assuming 'Current Club' and 'league' are the columns in players dataframe
team_league_mapping = combined_player_stats[['Current Club', 'league']].drop_duplicates()
team_league_mapping = team_league_mapping.set_index('Current Club')['league'].to_dict()

# Function to get league from team name
def get_league(team_name):
    return team_league_mapping.get(team_name, "Unknown")

# Apply the function to the matches dataframe
cleaned_matches_data_df['league'] = cleaned_matches_data_df['home_team_name'].apply(get_league)

# Check for matches with 'Unknown' league
unknown_league_matches = cleaned_matches_data_df[cleaned_matches_data_df['league'] == "Unknown"]
print(unknown_league_matches)

"""THIS IS USEFUL FOR TEAM AND PLAYER AGGREGATION

Results Visualization
Visualize the results of the model.
"""

import pandas as pd

# Example: Aggregating player stats for a team in a particular match
# Let's say you have a function that takes a team name and a date and returns the aggregated stats for that team
def get_team_stats_for_match(team_name, match_date, player_stats_df):
    # Filter player stats for the given team and date
    # Aggregate stats as needed (e.g., average rating, total goals)
    # Return aggregated stats as a pandas Series or DataFrame row
    pass

# Loop through each match in the match data
for index, row in cleaned_matches_data_df.iterrows():
    # Get team names and match date
    home_team = row['home_team_name']
    away_team = row['away_team_name']
    match_date = row['date_GMT']

    # Get aggregated stats for both teams
    home_team_stats = get_team_stats_for_match(home_team, match_date, combined_player_stats)
    away_team_stats = get_team_stats_for_match(away_team, match_date, combined_player_stats)

    # Merge these stats into the match data row
    # This can be done by setting new columns in 'row' or by merging Series/DataFrames

# Now 'cleaned_matches_data_df' will have additional columns for each team's player stats

# Function to calculate form
def calculate_form(df, num_matches):
    form = []
    for team in df['home_team_name'].unique():
        team_matches = df[(df['home_team_name'] == team) | (df['away_team_name'] == team)]
        team_matches = team_matches.sort_values('date_GMT').tail(num_matches)
        team_form = 0
        for index, row in team_matches.iterrows():
            if row['home_team_name'] == team:
                if row['home_team_goal_count'] > row['away_team_goal_count']:
                    team_form += 3  # Win
                elif row['home_team_goal_count'] == row['away_team_goal_count']:
                    team_form += 1  # Draw
            elif row['away_team_name'] == team:
                if row['away_team_goal_count'] > row['home_team_goal_count']:
                    team_form += 3  # Win
                elif row['away_team_goal_count'] == row['home_team_goal_count']:
                    team_form += 1  # Draw
        form.append({'team': team, 'form': team_form})
    return pd.DataFrame(form)

# Calculate form for teams
team_form_df = calculate_form(cleaned_matches_data_df, 5)

# Function to calculate form
def calculate_form(df, num_matches):
    form = []
    for team in df['away_team_name'].unique():
        team_matches = df[(df['home_team_name'] == team) | (df['away_team_name'] == team)]
        team_matches = team_matches.sort_values('date_GMT').tail(num_matches)
        team_form = 0
        for index, row in team_matches.iterrows():
            if row['home_team_name'] == team:
                if row['home_team_goal_count'] > row['away_team_goal_count']:
                    team_form += 3  # Win
                elif row['home_team_goal_count'] == row['away_team_goal_count']:
                    team_form += 1  # Draw
            elif row['away_team_name'] == team:
                if row['away_team_goal_count'] > row['home_team_goal_count']:
                    team_form += 3  # Win
                elif row['away_team_goal_count'] == row['home_team_goal_count']:
                    team_form += 1  # Draw
        form.append({'team': team, 'form': team_form})
    return pd.DataFrame(form)

# Calculate form for teams
team_form_df = calculate_form(cleaned_matches_data_df, 5)

def head_to_head_stats(df, team1, team2):
    matches = df[((df['home_team_name'] == team1) & (df['away_team_name'] == team2)) |
                 ((df['home_team_name'] == team2) & (df['away_team_name'] == team1))]
    # Perform calculations to find head-to-head stats
    # ...

# Example usage
head_to_head_stats(cleaned_matches_data_df, 'Team A', 'Team B')

team_form_df['form_average'] = team_form_df['form'] / 5  # Assuming 5 is the number of matches considered for form

def create_ftr_column(df):
    df['FTR'] = df.apply(lambda row: 0 if row['home_team_goal_count'] > row['away_team_goal_count'] else 1, axis=1)

create_ftr_column(cleaned_matches_data_df)

# Display the first few rows of the DataFrame
print(cleaned_matches_data_df.head())

# Replace 'Team A' and 'Team B' with actual team names
sample_teams = cleaned_matches_data_df[(cleaned_matches_data_df['home_team_name'] == 'Manchester City') |
                                       (cleaned_matches_data_df['away_team_name'] == 'Liverpool')]
print(sample_teams[['home_team_name', 'away_team_name', 'FTR',]])

# Print all column names
print(cleaned_matches_data_df.columns)

# Check if 'form' column exists
if 'form' in cleaned_matches_data_df.columns:
    print("Column 'form' exists in the DataFrame.")
else:
    print("Column 'form' does not exist in the DataFrame.")

try:
    form_column = cleaned_matches_data_df['form']
    print("Column 'form' exists.")
except KeyError:
    print("Column 'form' does not exist.")

def calculate_form(df, team_name, n_matches=5):
    """
    Calculate the form of a team over the last n matches.
    """
    recent_matches = df[df['home_team_name'] == team_name].tail(n_matches)
    points = 0
    for index, row in recent_matches.iterrows():
        if row['home_team_goal_count'] > row['away_team_goal_count']:
            points += 3  # Home win
        elif row['home_team_goal_count'] == row['away_team_goal_count']:
            points += 1  # Draw
    return points / (3 * n_matches)  # Return the average points per match

# Calculate form for each team and each match
cleaned_matches_data_df['home_team_form'] = cleaned_matches_data_df.apply(
    lambda row: calculate_form(cleaned_matches_data_df, row['home_team_name']), axis=1)

cleaned_matches_data_df['away_team_form'] = cleaned_matches_data_df.apply(
    lambda row: calculate_form(cleaned_matches_data_df, row['away_team_name']), axis=1)

def calculate_head_to_head_stats(df, home_team, away_team):
    """
    Calculate head-to-head statistics between two teams.
    """
    # Filter matches where either team was playing at home against the other
    head_to_head_matches = df[((df['home_team_name'] == home_team) & (df['away_team_name'] == away_team)) |
                              ((df['home_team_name'] == away_team) & (df['away_team_name'] == home_team))]

    # Calculate stats like number of wins, draws, losses, goals scored, etc.
    home_wins = len(head_to_head_matches[(head_to_head_matches['home_team_name'] == home_team) &
                                         (head_to_head_matches['home_team_goal_count'] > head_to_head_matches['away_team_goal_count'])])
    away_wins = len(head_to_head_matches[(head_to_head_matches['home_team_name'] == away_team) &
                                         (head_to_head_matches['home_team_goal_count'] < head_to_head_matches['away_team_goal_count'])])
    draws = len(head_to_head_matches[head_to_head_matches['home_team_goal_count'] == head_to_head_matches['away_team_goal_count']])

    # Return a dictionary of stats
    return {
        'home_wins': home_wins,
        'away_wins': away_wins,
        'draws': draws
    }

# Apply the function to each match
cleaned_matches_data_df['head_to_head'] = cleaned_matches_data_df.apply(
    lambda row: calculate_head_to_head_stats(cleaned_matches_data_df, row['home_team_name'], row['away_team_name']), axis=1)



def calculate_ftr(home_goals, away_goals):
    if home_goals > away_goals:
        return 0  # Home win
    else:
        return 1  # Draw or Away win

cleaned_matches_data_df['FTR'] = cleaned_matches_data_df.apply(
    lambda row: calculate_ftr(row['home_team_goal_count'], row['away_team_goal_count']), axis=1)

# Optionally, save your DataFrame to a CSV file
cleaned_matches_data_df.to_csv('/content/cleaned_matches_data_with_features.csv', index=False)

# Assuming you have already loaded cleaned_matches_data_df
print(cleaned_matches_data_df.describe())

# Histograms for continuous variables in matches data
cleaned_matches_data_df[['home_team_goal_count', 'away_team_goal_count']].hist(bins=15, figsize=(15, 6))

# Scatter Plot for relationships in matches data
plt.scatter(cleaned_matches_data_df['home_team_shots'], cleaned_matches_data_df['home_team_goal_count'])
plt.xlabel('Home Team Shots')
plt.ylabel('Home Team Goals')
plt.title('Home Team Shots vs Goals')

# Correlation Heatmap for matches data
corr_matches = cleaned_matches_data_df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr_matches, annot=True, fmt=".2f")

# Assuming you have already loaded combined_player_stats
print(combined_player_stats.describe())

# Histograms for continuous variables in player stats
combined_player_stats[['age', 'minutes_played_overall', 'goals_per_90_overall']].hist(bins=15, figsize=(15, 6))

# Scatter Plot for relationships in player stats
plt.scatter(combined_player_stats['goals_overall'], combined_player_stats['assists_overall'])
plt.xlabel('Goals')
plt.ylabel('Assists')
plt.title('Goals vs Assists in Player Stats')

# Correlation Heatmap for player stats
corr_players = combined_player_stats.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr_players, annot=True, fmt=".2f")

cleaned_matches_data_df['FTR'] = (cleaned_matches_data_df['home_team_goal_count'] <= cleaned_matches_data_df['away_team_goal_count']).astype(int)

# Assuming 'combined_player_stats' is your DataFrame with player stats
shot_related_columns = [col for col in combined_player_stats.columns if 'shot' in col.lower()]
print(shot_related_columns)

# Handling division by zero
combined_player_stats['shot_efficiency'] = combined_player_stats.apply(
    lambda row: row['goals_overall'] / row['shots_total_overall'] if row['shots_total_overall'] > 0 else 0, axis=1
)

# Shot Efficiency
combined_player_stats['shot_efficiency'] = combined_player_stats['goals_overall'] / combined_player_stats['shots_total_overall']

# Shots on Target Rate
combined_player_stats['shots_on_target_rate'] = combined_player_stats['shots_on_target_total_overall'] / combined_player_stats['shots_total_overall']

# Conversion Rate
combined_player_stats['conversion_rate'] = combined_player_stats['goals_overall'] / combined_player_stats['shots_on_target_total_overall']

# Shooting Accuracy
combined_player_stats['shooting_accuracy'] = combined_player_stats['shots_on_target_total_overall'] / combined_player_stats['shots_total_overall']

# Defensive Pressure
combined_player_stats['defensive_pressure'] = combined_player_stats['shots_faced_total_overall'] / combined_player_stats['appearances_overall']

# Goalkeeping Performance
# Assuming 'goals_conceded_overall' is the number of goals a goalkeeper has conceded
combined_player_stats['goalkeeper_performance'] = combined_player_stats['conceded_per_90_overall'] / combined_player_stats['shots_faced_total_overall']

# Long-Shot Specialist
combined_player_stats['long_shot_specialist'] = combined_player_stats['shots_total_overall'] - combined_player_stats['shots_on_target_total_overall']

# Shot Conversion Rate Standardized
shot_conversion_mean = combined_player_stats['shot_conversion_rate_overall'].mean()
shot_conversion_std = combined_player_stats['shot_conversion_rate_overall'].std()
combined_player_stats['shot_conversion_rate_standardized'] = (combined_player_stats['shot_conversion_rate_overall'] - shot_conversion_mean) / shot_conversion_std

# Before running this code, ensure that there are no divisions by zero and handle any NaN values that may result from these computations.

print(combined_player_stats.columns)

features = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']



# Selecting features - example features selected here
features = df[['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_ppg', 'away_ppg',
               'Home Team Pre-Match xG', 'Away Team Pre-Match xG',
               'average_goals_per_match_pre_match', 'btts_percentage_pre_match',
               'over_15_percentage_pre_match', 'over_25_percentage_pre_match',
               'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match',
               'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win',
               'league', 'home_team_form', 'away_team_form', 'head_to_head']]

# Target variable
target = df['FTR']

# Checking the selected features and target
print("Selected Features Sample:")
print(features.head())

print("\nTarget Variable Sample:")
print(target.head())



"""ADJUST DATE HERE FOR SPECIFIC SEARCH:"""

# Check if 'date' column exists in the DataFrame
if 'date_GMT' in df.columns:
    print("Column 'date_GMT' exists.")
else:
    print("Column 'date_GMT' does not exist.")

# Example: Sorting data by date instead of timestamp
df.sort_values(by='date_GMT', inplace=True)

# Example: Splitting data based on a specific date
train_df = df[df['date_GMT'] < '2023-11-30']
test_df = df[df['date_GMT'] >= '2023-11-30']



if 'head_to_head' in df.columns:
    print("Column 'head_to_head' exists.")
else:
    print("Column 'head_to_head' does not exist.")

# Assuming you have a function that can calculate head to head statistics
def calculate_head_to_head(home_team, away_team):
    # This function would return head to head stats like {'home_wins': X, 'away_wins': Y, 'draws': Z}
    # based on historical data of matches between the two teams.
    pass

# Apply the function to each row in the DataFrame
df['head_to_head'] = df.apply(lambda row: calculate_head_to_head(row['home_team_name'], row['away_team_name']), axis=1)

# Update categorical and numeric features
categorical_features = ['league']
numeric_features = [col for col in df.columns if col not in categorical_features + ['head_to_head']]

# Proceed with your preprocessing steps as before

# Display the first few rows of the DataFrame
print("First few rows of the DataFrame:")
print(df.head())

# Display summary statistics of the DataFrame
print("\nSummary Statistics:")
print(df.describe())

# Display the data types of each column
print("\nData Types:")
print(df.dtypes)



df['goal_difference'] = df['home_team_goal_count'] - df['away_team_goal_count']

df['combined_team_form'] = (df['home_team_form'] + df['away_team_form']) / 2

df['date_GMT'] = pd.to_datetime(df['date_GMT'])
df['day_of_week'] = df['date_GMT'].dt.day_name()
df['time_of_day'] = df['date_GMT'].dt.hour

df['date_GMT'] = pd.to_datetime(df['date_GMT'])



df = df.drop(['home_team_goal_timings', 'away_team_goal_timings', 'head_to_head'], axis=1)

print(df.columns)

cutoff_date = pd.Timestamp('2023-11-30')
train_data = df[df['date_GMT'] < cutoff_date]
test_data = df[df['date_GMT'] >= cutoff_date]

X_train = train_data.drop('FTR', axis=1)
y_train = train_data['FTR']
X_test = test_data.drop('FTR', axis=1)
y_test = test_data['FTR']

X_train = train_data.drop('FTR', axis=1)
y_train = train_data['FTR']
X_test = test_data.drop('FTR', axis=1)
y_test = test_data['FTR']



from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X = cleaned_matches_data_df[features]
y = cleaned_matches_data_df['FTR']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

df_encoded = pd.get_dummies(df, columns=['home_team_name', 'away_team_name', 'league', 'referee', 'stadium_name'])

df = df.drop(['date_GMT', 'home_team_goal_timings', 'away_team_goal_timings', 'head_to_head'], axis=1)

df = pd.get_dummies(df, columns=['status', 'day_of_week'])



# Commented out IPython magic to ensure Python compatibility.
# %whos



from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"Cross-validated scores: {scores}")

# Assuming 'favorite_based_on_odds' is the column that shows the expected winner based on odds
cleaned_matches_data_df['odds_accuracy'] = (cleaned_matches_data_df['favorite_based_on_odds'] == cleaned_matches_data_df['FTR']).astype(int)



categorical_columns = ['home_team_name', 'away_team_name', 'referee', 'stadium_name', 'league']
X_train = pd.get_dummies(X_train, columns=categorical_columns)

print(df.columns)

def determine_favorite(row):
    home_odds = row['odds_ft_home_team_win']
    draw_odds = row['odds_ft_draw']
    away_odds = row['odds_ft_away_team_win']

    if min(home_odds, draw_odds, away_odds) == home_odds:
        return 'Home'
    elif min(home_odds, draw_odds, away_odds) == away_odds:
        return 'Away'
    else:
        return 'Draw'

# Apply the function to each row
cleaned_matches_data_df['favorite_based_on_odds'] = cleaned_matches_data_df.apply(determine_favorite, axis=1)

from xgboost import XGBClassifier

# Initialize the model
model = XGBClassifier()

# Fit the model to your training data
model.fit(X_train, y_train)



print(X_test.dtypes)

y_pred = model.predict(X_test)

# Proceed with evaluating the model

from xgboost import XGBClassifier

# Initialize the model
model = XGBClassifier()

# Fit the model to your training data
model.fit(X_train, y_train)

# Making predictions
y_pred = model.predict(X_test)

# Evaluating the model
from sklearn.metrics import accuracy_score, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))



# Converting to Unix timestamp
X['date_GMT'] = X['date_GMT'].astype(int) / 10**9

object_columns = ['status', 'home_team_goal_timings', 'away_team_goal_timings', 'head_to_head', 'day_of_week']
X = pd.get_dummies(X, columns=object_columns)

from sklearn.model_selection import cross_val_score

model = XGBClassifier(enable_categorical=True)
scores = cross_val_score(model, X, y, cv=5, error_score='raise')
print("Cross-validated scores:", scores)
print("Average score:", scores.mean())

from xgboost import XGBClassifier

model = XGBClassifier()  # Replace with your model and parameters
print(model)

print(df.describe(include='all'))

from sklearn.metrics import classification_report, confusion_matrix

# Assuming y_test and y_pred are your test labels and model predictions
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)  # Replace X and y with your features and labels
print("Cross-validation scores:", scores)

print("Dataset shape:", df.shape)
print("Dataset info:")
print(df.info())

# Example: Comparing training and validation accuracy
train_accuracy = ...  # Your training accuracy
validation_accuracy = ...  # Your validation accuracy
print("Training accuracy:", train_accuracy)
print("Validation accuracy:", validation_accuracy)

from xgboost import XGBClassifier

model = XGBClassifier()  # Replace with your specific model parameters
model.fit(X_train, y_train)

import pandas as pd

# Assuming df is your original DataFrame
# And it has columns like 'home_team_name', 'away_team_name', 'match_outcome'

# Initialize columns
df['home_team_recent_form'] = 0
df['away_team_recent_form'] = 0

# Iterate and calculate form (example for home team)
for index, row in df.iterrows():
    # Get last 3-5 matches for the home team
    recent_matches = df[(df['home_team_name'] == row['home_team_name']) & (df.index < index)].tail(3)

    # Calculate wins, draws, and losses
    wins = len(recent_matches[recent_matches['FTR'] == 'Win'])
    draws = len(recent_matches[recent_matches['FTR'] == 'Draw'])
    losses = len(recent_matches[recent_matches['FTR'] == 'Loss'])

    # Assign to the new column
    df.at[index, 'home_team_recent_form'] = wins - losses  # Example formula

# Repeat similar steps for the away team

feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=X_train.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)
print(feature_importances)

import pandas as pd

# Assuming df is your DataFrame
print(df.columns)



"""above is the features of the cleaned matches data df.csv"""

was the features we want to drop

"""next is the features we want to drop"""

# Assuming 'df' is your DataFrame

# List of features to drop
features_to_drop = ['timestamp', 'home_team_name', 'away_team_name', 'referee',
                    'stadium_name', 'league', 'status_complete', 'status_incomplete',
                    'day_of_week_Thursday', 'day_of_week_Tuesday']

# Drop the features
df = df.drop(features_to_drop, axis=1)

# Define your features (X) and target variable (y)
X = df.drop('FTR', axis=1)  # Replace 'FTR' with your actual target column name if different
y = df['FTR']



from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model = XGBClassifier()
model.fit(X_train, y_train)

# Making predictions and evaluating the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.model_selection import cross_val_score

# Using cross-validation to assess model performance
scores = cross_val_score(model, X, y, cv=5)
print("Cross-validated scores:", scores)

feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=X_train.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)
print(feature_importances)

# Assuming feature_importances is your DataFrame with feature importances
zero_importance_features = feature_importances[feature_importances['importance'] == 0].index

# Drop these features from your dataset
X = X.drop(zero_importance_features, axis=1)



scores = cross_val_score(model, X, y, cv=5)
print("Cross-validated scores:", scores)

# Calculating average possession for home teams
df['average_home_possession'] = df.groupby('home_team_name')['home_team_possession'].transform('mean')

# Calculating average possession for away teams
df['average_away_possession'] = df.groupby('away_team_name')['away_team_possession'].transform('mean')



import pandas as pd

file_path = '/content/cleaned_matches_data_df.csv'  # Update with your file's path
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
print(df.head())

# Display DataFrame information
print(df.info())

# Check summary statistics
print(df.describe())

# Fill NA values in 'date_GMT' with a placeholder
df['date_GMT'].fillna('No Date', inplace=True)

# Filter for matches on November 28th and 29th, 2023
matches_nov_28_29 = df[df['date_GMT'].str.contains('Nov 28 2023|Nov 29 2023')]

# Commented out IPython magic to ensure Python compatibility.
# List only the variables that are DataFrames
# %whos DataFrame



import pickle

model_file_path = '/content/trained_xgboost_model.pkl'  # Update this to the correct path

with open(model_file_path, 'rb') as file:
    xgboost_model = pickle.load(file)

print(df.columns)

# Updated list of features to drop
features_to_drop = ['timestamp', 'home_team_name', 'away_team_name']

# Drop the features
df = df.drop(features_to_drop, axis=1)

# Define your features (X) and target variable (y)
X = df.drop('FTR_encoded', axis=1)  # Assuming 'FTR_encoded' is your target column
y = df['FTR']



from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable
y_encoded = label_encoder.fit_transform(y)

# Now y_encoded contains the encoded labels as integers

import pandas as pd

# Load the data
file_path = '/content/cleaned_matches_data_with_features.csv'  # Path to your file
df = pd.read_csv(file_path)

# Display the first few rows to confirm it's loaded correctly
print(df.head())

cutoff_date = '2023-11-30'
train_data = df[df['date_GMT'] < cutoff_date]
test_data = df[df['date_GMT'] >= cutoff_date]

"""Results Visualization
Visualize the results of the model.
"""

# Drop columns that won't be used as features
columns_to_drop = ['timestamp', 'date_GMT', 'status', 'referee', 'stadium_name', 'league']
X_train = train_data.drop(columns=columns_to_drop + ['FTR'], axis=1)
y_train = train_data['FTR']
X_test = test_data.drop(columns=columns_to_drop + ['FTR'], axis=1)
y_test = test_data['FTR']

import pandas as pd

# Load your dataset
df = pd.read_csv('/content/cleaned_matches_data_with_features.csv')

# Convert 'date_GMT' to a datetime object
df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Define your cutoff date
cutoff_date = pd.Timestamp('2023-11-30')

# Split the data
train_data = df[df['date_GMT'] < cutoff_date]
test_data = df[df['date_GMT'] >= cutoff_date]

# Check if data splitting is correct
print("Training data shape:", train_data.shape)
print("Testing data shape:", test_data.shape)

# Proceed with feature selection and model training

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Selecting features for the model
feature_columns = ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
target_column = 'FTR'  # Replace with your target column

# Splitting the features and target
X_train = train_data[feature_columns]
y_train = train_data[target_column]
X_test = test_data[feature_columns]
y_test = test_data[target_column]

# Defining preprocessing for numeric and categorical features
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Creating a pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', LogisticRegression())])

# Training the model
pipeline.fit(X_train, y_train)

# Predicting on the test set
y_pred = pipeline.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")





# Assuming 'cleaned_matches_data_with_features.csv' is loaded into 'df'
# Replace 'features_list' with the list of feature column names you want to include
features_list =  ['Pre-Match PPG (Home)', 'Pre-Match PPG (Away)', 'home_team_goal_count', 'away_team_goal_count', 'average_corners_per_match_pre_match', 'average_cards_per_match_pre_match', 'odds_ft_home_team_win', 'odds_ft_draw', 'odds_ft_away_team_win']
X = df[features_list]
y = df['FTR']  # Or your target column

# Now run the cross-validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(model, X, y, cv=5)

print("Cross-validation scores:", cv_scores)
print("Mean CV score:", cv_scores.mean())



print("Training Features:", X_train.columns.tolist())
print("Testing Features:", X_test.columns.tolist())



# Step 1: Compare feature lists
print("Features in X_train:", X_train.columns.tolist())
print("\nFeatures in X_test:", X_test.columns.tolist())

# Step 2: Identify and add missing features in X_test
missing_features = [feature for feature in X_train.columns if feature not in X_test.columns]
for feature in missing_features:
    X_test[feature] = 0  # or any other appropriate default value

# Step 3: Align feature order in X_test to match X_train
X_test = X_test[X_train.columns]

# Step 4: Check and align data types if necessary
for column in X_train.columns:
    if X_train[column].dtype != X_test[column].dtype:
        X_test[column] = X_test[column].astype(X_train[column].dtype)

# Now try predicting again
try:
    y_pred = model.predict(X_test)
    print("Prediction successful.")
except Exception as e:
    print("Error during prediction:", e)

# If the prediction is successful, you can proceed with evaluation
if 'y_pred' in locals():
    print(classification_report(y_test, y_pred))

# Train the model with the current set of features in X_train
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# Check the distribution of classes in the test set
print(y_test.value_counts())



from google.colab import drive
drive.mount('/content/drive')

import os

folder_path = '/content/drive/My Drive/94_betting_model'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# Now save the model
joblib.dump(model, folder_path + '/94_betting_model.pkl')

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)
# Similarly for other DataFrames or relevant objects

print(cleaned_matches_data_df.head())

folder_path = '/content/drive/My Drive/94_betting_model'  # Replace with your folder path

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)

# Predict on the test set
y_pred = model.predict(X_test)

# Add predictions to the test DataFrame
test_data['Predicted_FTR'] = y_pred

# Filter the DataFrame for the specific dates
games_on_nov_30 = test_data[test_data['date_GMT'].dt.date == pd.to_datetime('2023-11-30').date()]
games_on_dec_01 = test_data[test_data['date_GMT'].dt.date == pd.to_datetime('2023-12-01').date()]

# Display the predictions for these dates
print("Predictions for Games on November 30, 2023:")
print(games_on_nov_30[['home_team_name', 'away_team_name', 'Predicted_FTR']])

print("\nPredictions for Games on December 01, 2023:")
print(games_on_dec_01[['home_team_name', 'away_team_name', 'Predicted_FTR']])





"""# ***PREDICTIONS!!!!!!***"""

cleaned_matches_data_df = pd.read_csv('/content/cleaned_matches_data_df.csv')

cleaned_matches_data_df.to_csv(folder_path + '/cleaned_matches_data_df.csv', index=False)

folder_path = '/content/drive/My Drive/94_betting_model'



# Check for unique values or formats in 'date_GMT'
print(df['date_GMT'].unique())

# Option 1: Drop rows where 'date_GMT' is not in a standard date format
# df = df[df['date_GMT'] != 'No Date present']

# Option 2: Replace non-standard dates with a default date or NaN
# df['date_GMT'] = df['date_GMT'].replace('No Date present', pd.NaT)  # or some default date

# After cleaning, convert 'date_GMT' to datetime
X['date_GMT'] = pd.to_datetime(X['date_GMT'], errors='coerce')  # 'coerce' will set invalid parsing as NaT

# Convert 'date_GMT' to datetime and then extract numerical features
X['date_GMT'] = pd.to_datetime(X['date_GMT'], errors='coerce')
X['year'] = X['date_GMT'].dt.year
X['month'] = X['date_GMT'].dt.month
X['day'] = X['date_GMT'].dt.day
X = X.drop('date_GMT', axis=1)  # Drop the original 'date_GMT' column

# Ensure 'FTR' is not in your features DataFrame
X = X.drop('FTR', axis=1, errors='ignore')



# Convert 'date_GMT' to datetime, setting errors to 'coerce' will turn invalid parsing into NaT
df['date_GMT'] = pd.to_datetime(df['date_GMT'], errors='coerce')

# Drop rows where 'date_GMT' is NaT (not a time)
df = df.dropna(subset=['date_GMT'])

from sklearn.metrics import classification_report

# Now, use classification_report to evaluate your model
print(classification_report(y_test_encoded, y_pred))

print(y_train.value_counts())
print(y_test.value_counts())



# Extract year, month, and day as separate features
X_train['year'] = X_train['date_GMT'].dt.year
X_train['month'] = X_train['date_GMT'].dt.month
X_train['day'] = X_train['date_GMT'].dt.day

X_test['year'] = X_test['date_GMT'].dt.year
X_test['month'] = X_test['date_GMT'].dt.month
X_test['day'] = X_test['date_GMT'].dt.day

# Drop the original 'date_GMT' column
X_train = X_train.drop('date_GMT', axis=1)
X_test = X_test.drop('date_GMT', axis=1)

model.fit(X_train, y_train)



# Convert 'date_GMT' to datetime and then extract year, month, and day
X_train['year'] = X_train['date_GMT'].dt.year
X_train['month'] = X_train['date_GMT'].dt.month
X_train['day'] = X_train['date_GMT'].dt.day
X_train = X_train.drop('date_GMT', axis=1)  # Drop the original 'date_GMT' column

# Repeat for X_test if you have a test set
X_test['year'] = X_test['date_GMT'].dt.year
X_test['month'] = X_test['date_GMT'].dt.month
X_test['day'] = X_test['date_GMT'].dt.day
X_test = X_test.drop('date_GMT', axis=1)

model.fit(X_train, y_train)

# Assuming X_train and y_train are already defined and preprocessed
model.fit(X_train, y_train)

# Assuming X_test and y_test are your testing data
y_pred = model.predict(X_test)

# Evaluate the model's performance
print(classification_report(y_test, y_pred))

import pandas as pd

# Assuming 'model' is your trained XGBoost model
feature_importances = model.feature_importances_

# Assuming 'X_train' is your training data with feature names
feature_names = X_train.columns

# Create a DataFrame to display feature names and their importances
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort the DataFrame by importance in descending order
importances_df = importances_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(importances_df)

df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Splitting data: Training data is before November 28th
train_data = df[df['date_GMT'] < '2023-11-28']

# Testing data is on or after November 28th
test_data = df[df['date_GMT'] >= '2023-11-28']

# Assuming 'FTR' is the target variable
X_train = train_data.drop(['FTR', 'date_GMT'], axis=1)  # Dropping 'date_GMT' if not used as a feature
y_train = train_data['FTR']

X_test = test_data.drop(['FTR', 'date_GMT'], axis=1)
y_test = test_data['FTR']

model.fit(X_train, y_train)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("First few rows of X_train:\n", X_train.head())

import pandas as pd

# Load your data into a DataFrame
df = pd.read_csv('/content/cleaned_matches_data_with_features.csv')

# Data cleaning and preprocessing steps
# ...

# Convert 'date_GMT' to datetime for filtering
df['date_GMT'] = pd.to_datetime(df['date_GMT'])

# Split the data
train_data = df[df['date_GMT'] < '2023-11-28']
test_data = df[df['date_GMT'] >= '2023-11-28']

# Prepare features and labels
X_train = train_data.drop(['FTR', 'date_GMT'], axis=1)  # Assuming 'FTR' is your target
y_train = train_data['FTR']
X_test = test_data.drop(['FTR', 'date_GMT'], axis=1)
y_test = test_data['FTR']

categorical_cols = ['status', 'home_team_name', 'away_team_name', 'referee',
                    'home_team_goal_timings', 'away_team_goal_timings',
                    'stadium_name', 'league', 'head_to_head']

df = pd.get_dummies(df, columns=categorical_cols)



print(df.columns)

selected_features = [
    'home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win',
    'home_team_possession', 'home_team_shots', 'over_25_percentage_pre_match',
    'team_a_xg', 'average_corners_per_match_pre_match', 'away_team_goal_count',
    'away_team_shots_off_target', 'away_team_goal_count_half_time', 'home_ppg',
    'away_team_shots_on_target', 'home_team_fouls', 'draws_head_to_head',
    'average_home_possession', 'home_team_form', 'Home Team Pre-Match xG',
    'odds_ft_over45', 'Pre-Match PPG (Home)', 'Pre-Match PPG (Away)',
    'Game Week', 'home_team_shots_off_target', 'average_cards_per_match_pre_match'
]



print(df.columns)

selected_features = [
    'home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win',
    'home_team_possession', 'home_team_shots', 'over_25_percentage_pre_match',
    'team_a_xg', 'average_corners_per_match_pre_match', 'away_team_goal_count',
    'away_team_shots_off_target', 'away_team_goal_count_half_time', 'home_ppg',
    'away_team_shots_on_target', 'home_team_fouls',
    'Home Team Pre-Match xG', 'odds_ft_over45', 'Pre-Match PPG (Home)',
    'Pre-Match PPG (Away)', 'Game Week', 'home_team_shots_off_target',
    'average_cards_per_match_pre_match'
]

X = df[selected_features]
y = df['FTR']  # Replace with your actual target column name

# Continue with model training and evaluation
# ...

import pandas as pd

# Assuming 'df' is your DataFrame
# List all columns that are of object type, which are typically considered categorical
categorical_features = df.select_dtypes(include=['object']).columns

# Print the list of categorical features
print("Categorical Features:", list(categorical_features))

import pandas as pd

# Convert 'birthday_GMT' to datetime and extract year
df['birthday_GMT'] = pd.to_datetime(df['birthday_GMT'])
df['birth_year'] = df['birthday_GMT'].dt.year

# One-hot encode low cardinality categorical features
df = pd.get_dummies(df, columns=['league', 'season', 'position', 'age_group'])

# Label encode high cardinality features
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in ['full_name', 'Current Club', 'nationality', 'player_id']:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

# Drop the original 'birthday_GMT' column
df = df.drop('birthday_GMT', axis=1)

# Now df is ready for model training



# Assuming 'df' is your DataFrame
print(df.head())

from sklearn.metrics import classification_report

# Now you can use classification_report without any error
print(classification_report(y_test_encoded, y_pred))

df

# Assuming your dataframe is named 'df' and the date_GMT column is named 'date_GMT'
# Replace 'start_date' and 'end_date' with your specific date range

start_date = '2023-11-28'
end_date = '2023-12-10'

# Filter the dataframe based on the date range
df = df[(df['date_GMT'] >= start_date) & (df['date_GMT'] <= end_date)]

# Display the filtered dataframe
df

df.head(30)

# Save the dataframe to a CSV file
df.to_csv('filtered_data.csv', index=False)

# Save the dataframe to a CSV file
df.to_csv('filtered_data.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('filtered_data.csv')



# Commented out IPython magic to ensure Python compatibility.
# List only the variables that are DataFrames
# %whos

import pickle

# Correct path to your saved XGBoost model
xgboost_model_path = '/content/drive/My Drive/trained_xgboost_model.pkl'

# Load the XGBoost model
try:
    with open(xgboost_model_path, 'rb') as file:
        xgboost_model = pickle.load(file)
    print("XGBoost model loaded successfully.")
except FileNotFoundError:
    print("Model file not found. Please check the file path.")
except Exception as e:
    print(f"An error occurred while loading the XGBoost model: {e}")



# Filter the DataFrame for matches on November 28th and 29th
nov_28_29_matches = df[df['date_GMT'].str.contains('Nov 28 2023|Nov 29 2023', na=False)]

# Display the first few rows to verify
print(nov_28_29_matches.head())

# Prepare the feature set for prediction
X_nov_28_29 = nov_28_29_matches[features]  # Assuming 'features' is a list of your model's feature names

# Verify the prepared data
print(X_nov_28_29.head())

with open(model_file_path, 'wb') as file:
    pickle.dump(model, file)



print(df.columns)

PREDICTIONS CONTINUED

"""PREDICTIONS CONTINUED"""



# Assuming 'df' is your DataFrame
df['FTR'] = (df['home_team_goal_count'] - df['away_team_goal_count']).apply(
    lambda x: 'H' if x > 0 else ('D' if x == 0 else 'A')
)

# Feature columns (excluding target-related columns)
feature_columns = [col for col in df.columns if col not in ['home_team_goal_count', 'away_team_goal_count', 'FTR']]

# Target column
target_column = 'FTR'

# Split the DataFrame into features (X) and target (y)
X_train = df[feature_columns]
y_train = df[target_column]

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'FTR' column to numerical classes
df['FTR_encoded'] = label_encoder.fit_transform(df['FTR'])

# Define y_train using the encoded FTR column
y_train = df['FTR_encoded']

print("Unique values in y_train:", set(y_train))

import pickle

# Assuming your trained model is stored in a variable named 'model'
model_file_path = '/content/drive/My Drive/trained_xgboost_model.pkl'  # Adjust the path as needed

# Save the model to a file
with open(model_file_path, 'wb') as file:
    pickle.dump(model, file)

# Selecting the required features for prediction
features_for_prediction = ['home_team_goal_count', 'away_team_shots', 'odds_ft_away_team_win']
X_new = df_new[features_for_prediction]

# Ensure X_new is preprocessed similarly to your training data
# (Add any preprocessing steps here if required)

import pickle
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to the saved model file
model_file_path = '/content/drive/My Drive/trained_xgboost_model.pkl'

# Load the model
with open(model_file_path, 'rb') as file:
    loaded_model = pickle.load(file)

!pip install xgboost
!pip install scikit-learn --upgrade

import pickle

# Paths to your model files
xgboost_model_path = '/content/drive/My Drive/trained_xgboost_model.pkl'
betting_model_path = '/content/drive/My Drive/betting_model_v1.pkl'

# Load the XGBoost model
try:
    with open(xgboost_model_path, 'rb') as file:
        xgboost_model = pickle.load(file)
    print("XGBoost model loaded successfully.")
except Exception as e:
    print(f"An error occurred while loading the XGBoost model: {e}")

# Load the betting model
try:
    with open(betting_model_path, 'rb') as file:
        betting_model = pickle.load(file)
    print("Betting model loaded successfully.")
except Exception as e:
    print(f"An error occurred while loading the betting model: {e}")



# Example: Filtering for matches on a specific date (adjust the condition as needed)
upcoming_matches = df[df['date_GMT'].str.contains('Upcoming_Date', na=False)]

# Define the file path for the new CSV file
new_data_file_path = '/content/drive/My Drive/upcoming_matches.csv'  # Adjust the path as needed

# Save the filtered data to a CSV file
upcoming_matches.to_csv(new_data_file_path, index=False)

# Load the new data for prediction
df_new = pd.read_csv(new_data_file_path)

# Prepare the features for prediction (ensure the same preprocessing as your training data)
# ...

# Load your trained model and make predictions
# ...



import shap

# Create a SHAP explainer and calculate SHAP values
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_train)

# Summary plot
shap.summary_plot(shap_values, X_train)

from sklearn.model_selection import GridSearchCV

# Define a set of parameters to test
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    # ... add other parameters here
}

# Initialize the Grid Search
grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='accuracy')

# Perform Grid Search
grid_search.fit(X_train, y_train)

# Best parameters
print("Best parameters:", grid_search.best_params_)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# Define a distribution of parameters to test
param_dist = {
    'n_estimators': randint(100, 500),
    'learning_rate': uniform(0.01, 0.2),
    # ... add other parameters here
}

# Initialize the Random Search
random_search = RandomizedSearchCV(XGBClassifier(), param_dist, n_iter=25, cv=3, scoring='accuracy')

# Perform Random Search
random_search.fit(X_train, y_train)

# Best parameters
print("Best parameters:", random_search.best_params_)

# Assuming 'match_outcome' is your target variable
correlation_matrix = df.corr()
print(correlation_matrix['FTR'].sort_values(ascending=False))

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Selecting features and target
features = ['home_team_recent_form', 'away_team_recent_form',
            'average_home_possession', 'average_away_possession',
            # ... include other relevant features
           ]
X = df[features]
y = df['FTR']

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the model
model = XGBClassifier()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

df.drop('home_wins_head_to_head', axis=1, inplace=True)

# Convert the 'favorite_based_on_odds' to a binary outcome similar to 'FTR'
# Assuming 'Home' win is 0 and 'Away' win or 'Draw' is 1
cleaned_matches_data_df['favorite_binary'] = (cleaned_matches_data_df['favorite_based_on_odds'] != 'Home').astype(int)

# Calculate odds accuracy
cleaned_matches_data_df['odds_accuracy'] = (cleaned_matches_data_df['favorite_binary'] == cleaned_matches_data_df['FTR']).astype(int)

def calculate_recent_form(df, team, num_games=3):
    recent_games = df[((df['home_team_name'] == team) | (df['away_team_name'] == team))].tail(num_games)
    form = 0
    for _, game in recent_games.iterrows():
        if game['home_team_name'] == team:
            if game['FTR'] == 'H':  # Assuming 'H' represents a home win
                form += 1
            elif game['FTR'] == 'A':  # Assuming 'A' represents an away win
                form -= 1
        else:
            if game['FTR'] == 'A':  # Away team wins
                form += 1
            elif game['FTR'] == 'H':  # Home team wins
                form -= 1
    return form

# Applying the function to calculate recent form
df['home_team_recent_form'] = df.apply(lambda row: calculate_recent_form(df, row['home_team_name']), axis=1)
df['away_team_recent_form'] = df.apply(lambda row: calculate_recent_form(df, row['away_team_name']), axis=1)

# Selecting categorical columns
categorical_cols = ['home_team_name', 'away_team_name', 'referee', 'stadium_name', 'league']

# Applying one-hot encoding
X_train = pd.get_dummies(X_train, columns=categorical_cols)
X_test = pd.get_dummies(X_test, columns=categorical_cols)

# Aligning columns of train and test set
X_train, X_test = X_train.align(X_test, join='inner', axis=1)  # This ensures both sets have the same dummy columns

model = XGBClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))



# Selecting categorical columns
categorical_cols = ['home_team_name', 'away_team_name', 'referee', 'stadium_name', 'league']

# Applying one-hot encoding to the entire dataset
X_encoded = pd.get_dummies(X, columns=categorical_cols)

# Now perform cross-validation
from sklearn.model_selection import cross_val_score
model = XGBClassifier()
scores = cross_val_score(model, X_encoded, y, cv=5)
print("Cross-validated scores:", scores)

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)  # Make sure X_train and y_train are already defined



import pandas as pd

# Get feature importances
feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=X_train.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)

# Display the feature importances
print(feature_importances)

# Assuming 'favorite_based_on_odds' is the column that shows the expected winner based on odds
cleaned_matches_data_df['odds_accuracy'] = (cleaned_matches_data_df['favorite_based_on_odds'] == cleaned_matches_data_df['FTR']).astype(int)

incorrect_odds_df = cleaned_matches_data_df[cleaned_matches_data_df['odds_accuracy'] == 0]
# Perform analysis on 'incorrect_odds_df' to find patterns